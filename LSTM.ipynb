{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiZ08mm7tH5S"
      },
      "source": [
        "# Load Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSPODo3GtNPl"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras import layers, losses\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.callbacks import TensorBoard\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5szuseYtUfm"
      },
      "source": [
        "# Loading and Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "es4VW6Hjtahh",
        "outputId": "30f4a477-04a5-45ed-951c-23dff4b29335"
      },
      "source": [
        "# Set a random seed for reproducibility.\r\n",
        "np.random.seed(1145)\r\n",
        "# Download the dataset\r\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\r\n",
        "raw_data = dataframe.values\r\n",
        "dataframe.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112522</td>\n",
              "      <td>-2.827204</td>\n",
              "      <td>-3.773897</td>\n",
              "      <td>-4.349751</td>\n",
              "      <td>-4.376041</td>\n",
              "      <td>-3.474986</td>\n",
              "      <td>-2.181408</td>\n",
              "      <td>-1.818287</td>\n",
              "      <td>-1.250522</td>\n",
              "      <td>-0.477492</td>\n",
              "      <td>-0.363808</td>\n",
              "      <td>-0.491957</td>\n",
              "      <td>-0.421855</td>\n",
              "      <td>-0.309201</td>\n",
              "      <td>-0.495939</td>\n",
              "      <td>-0.342119</td>\n",
              "      <td>-0.355336</td>\n",
              "      <td>-0.367913</td>\n",
              "      <td>-0.316503</td>\n",
              "      <td>-0.412374</td>\n",
              "      <td>-0.471672</td>\n",
              "      <td>-0.413458</td>\n",
              "      <td>-0.364617</td>\n",
              "      <td>-0.449298</td>\n",
              "      <td>-0.471419</td>\n",
              "      <td>-0.424777</td>\n",
              "      <td>-0.462517</td>\n",
              "      <td>-0.552472</td>\n",
              "      <td>-0.475375</td>\n",
              "      <td>-0.694200</td>\n",
              "      <td>-0.701868</td>\n",
              "      <td>-0.593812</td>\n",
              "      <td>-0.660684</td>\n",
              "      <td>-0.713831</td>\n",
              "      <td>-0.769807</td>\n",
              "      <td>-0.672282</td>\n",
              "      <td>-0.653676</td>\n",
              "      <td>-0.639406</td>\n",
              "      <td>-0.559302</td>\n",
              "      <td>-0.591670</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258179</td>\n",
              "      <td>1.433789</td>\n",
              "      <td>1.700533</td>\n",
              "      <td>1.999043</td>\n",
              "      <td>2.125341</td>\n",
              "      <td>1.993291</td>\n",
              "      <td>1.932246</td>\n",
              "      <td>1.797437</td>\n",
              "      <td>1.522284</td>\n",
              "      <td>1.251168</td>\n",
              "      <td>0.998730</td>\n",
              "      <td>0.483722</td>\n",
              "      <td>0.023132</td>\n",
              "      <td>-0.194914</td>\n",
              "      <td>-0.220917</td>\n",
              "      <td>-0.243737</td>\n",
              "      <td>-0.254695</td>\n",
              "      <td>-0.291136</td>\n",
              "      <td>-0.256490</td>\n",
              "      <td>-0.227874</td>\n",
              "      <td>-0.322423</td>\n",
              "      <td>-0.289286</td>\n",
              "      <td>-0.318170</td>\n",
              "      <td>-0.363654</td>\n",
              "      <td>-0.393456</td>\n",
              "      <td>-0.266419</td>\n",
              "      <td>-0.256823</td>\n",
              "      <td>-0.288694</td>\n",
              "      <td>-0.162338</td>\n",
              "      <td>0.160348</td>\n",
              "      <td>0.792168</td>\n",
              "      <td>0.933541</td>\n",
              "      <td>0.796958</td>\n",
              "      <td>0.578621</td>\n",
              "      <td>0.257740</td>\n",
              "      <td>0.228077</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.925286</td>\n",
              "      <td>0.193137</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.100878</td>\n",
              "      <td>-3.996840</td>\n",
              "      <td>-4.285843</td>\n",
              "      <td>-4.506579</td>\n",
              "      <td>-4.022377</td>\n",
              "      <td>-3.234368</td>\n",
              "      <td>-1.566126</td>\n",
              "      <td>-0.992258</td>\n",
              "      <td>-0.754680</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>0.148951</td>\n",
              "      <td>0.183527</td>\n",
              "      <td>0.294876</td>\n",
              "      <td>0.190233</td>\n",
              "      <td>0.235575</td>\n",
              "      <td>0.253487</td>\n",
              "      <td>0.221742</td>\n",
              "      <td>0.050233</td>\n",
              "      <td>0.178042</td>\n",
              "      <td>0.139563</td>\n",
              "      <td>0.046794</td>\n",
              "      <td>0.043007</td>\n",
              "      <td>0.106544</td>\n",
              "      <td>0.012654</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>0.045724</td>\n",
              "      <td>-0.045999</td>\n",
              "      <td>-0.072667</td>\n",
              "      <td>-0.071078</td>\n",
              "      <td>-0.153866</td>\n",
              "      <td>-0.227254</td>\n",
              "      <td>-0.249270</td>\n",
              "      <td>-0.253489</td>\n",
              "      <td>-0.332835</td>\n",
              "      <td>-0.264330</td>\n",
              "      <td>-0.345825</td>\n",
              "      <td>-0.310781</td>\n",
              "      <td>-0.334160</td>\n",
              "      <td>-0.306178</td>\n",
              "      <td>-0.174563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.808428</td>\n",
              "      <td>2.164346</td>\n",
              "      <td>2.070747</td>\n",
              "      <td>1.903614</td>\n",
              "      <td>1.764455</td>\n",
              "      <td>1.507769</td>\n",
              "      <td>1.293428</td>\n",
              "      <td>0.894562</td>\n",
              "      <td>0.578016</td>\n",
              "      <td>0.244343</td>\n",
              "      <td>-0.286443</td>\n",
              "      <td>-0.515881</td>\n",
              "      <td>-0.732707</td>\n",
              "      <td>-0.832465</td>\n",
              "      <td>-0.803318</td>\n",
              "      <td>-0.836252</td>\n",
              "      <td>-0.777865</td>\n",
              "      <td>-0.774753</td>\n",
              "      <td>-0.733404</td>\n",
              "      <td>-0.721386</td>\n",
              "      <td>-0.832095</td>\n",
              "      <td>-0.711982</td>\n",
              "      <td>-0.751867</td>\n",
              "      <td>-0.757720</td>\n",
              "      <td>-0.853120</td>\n",
              "      <td>-0.766988</td>\n",
              "      <td>-0.688161</td>\n",
              "      <td>-0.519923</td>\n",
              "      <td>0.039406</td>\n",
              "      <td>0.560327</td>\n",
              "      <td>0.538356</td>\n",
              "      <td>0.656881</td>\n",
              "      <td>0.787490</td>\n",
              "      <td>0.724046</td>\n",
              "      <td>0.555784</td>\n",
              "      <td>0.476333</td>\n",
              "      <td>0.773820</td>\n",
              "      <td>1.119621</td>\n",
              "      <td>-1.436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.567088</td>\n",
              "      <td>-2.593450</td>\n",
              "      <td>-3.874230</td>\n",
              "      <td>-4.584095</td>\n",
              "      <td>-4.187449</td>\n",
              "      <td>-3.151462</td>\n",
              "      <td>-1.742940</td>\n",
              "      <td>-1.490658</td>\n",
              "      <td>-1.183580</td>\n",
              "      <td>-0.394229</td>\n",
              "      <td>-0.282897</td>\n",
              "      <td>-0.356926</td>\n",
              "      <td>-0.287297</td>\n",
              "      <td>-0.399489</td>\n",
              "      <td>-0.473244</td>\n",
              "      <td>-0.379048</td>\n",
              "      <td>-0.399039</td>\n",
              "      <td>-0.178594</td>\n",
              "      <td>-0.339522</td>\n",
              "      <td>-0.498447</td>\n",
              "      <td>-0.337251</td>\n",
              "      <td>-0.425480</td>\n",
              "      <td>-0.423952</td>\n",
              "      <td>-0.463170</td>\n",
              "      <td>-0.493253</td>\n",
              "      <td>-0.549749</td>\n",
              "      <td>-0.529831</td>\n",
              "      <td>-0.530935</td>\n",
              "      <td>-0.502365</td>\n",
              "      <td>-0.417368</td>\n",
              "      <td>-0.526346</td>\n",
              "      <td>-0.471005</td>\n",
              "      <td>-0.676784</td>\n",
              "      <td>-0.898612</td>\n",
              "      <td>-0.610571</td>\n",
              "      <td>-0.530164</td>\n",
              "      <td>-0.765674</td>\n",
              "      <td>-0.581937</td>\n",
              "      <td>-0.537848</td>\n",
              "      <td>-0.556386</td>\n",
              "      <td>...</td>\n",
              "      <td>1.810988</td>\n",
              "      <td>2.185398</td>\n",
              "      <td>2.262985</td>\n",
              "      <td>2.052920</td>\n",
              "      <td>1.890488</td>\n",
              "      <td>1.793033</td>\n",
              "      <td>1.564784</td>\n",
              "      <td>1.234619</td>\n",
              "      <td>0.900302</td>\n",
              "      <td>0.551957</td>\n",
              "      <td>0.258222</td>\n",
              "      <td>-0.128587</td>\n",
              "      <td>-0.092585</td>\n",
              "      <td>-0.168606</td>\n",
              "      <td>-0.495989</td>\n",
              "      <td>-0.395034</td>\n",
              "      <td>-0.328238</td>\n",
              "      <td>-0.448138</td>\n",
              "      <td>-0.268230</td>\n",
              "      <td>-0.456415</td>\n",
              "      <td>-0.357867</td>\n",
              "      <td>-0.317508</td>\n",
              "      <td>-0.434112</td>\n",
              "      <td>-0.549203</td>\n",
              "      <td>-0.324615</td>\n",
              "      <td>-0.268082</td>\n",
              "      <td>-0.220384</td>\n",
              "      <td>-0.117429</td>\n",
              "      <td>0.614059</td>\n",
              "      <td>1.284825</td>\n",
              "      <td>0.886073</td>\n",
              "      <td>0.531452</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-0.713683</td>\n",
              "      <td>-0.532197</td>\n",
              "      <td>0.321097</td>\n",
              "      <td>0.904227</td>\n",
              "      <td>-0.421797</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490473</td>\n",
              "      <td>-1.914407</td>\n",
              "      <td>-3.616364</td>\n",
              "      <td>-4.318823</td>\n",
              "      <td>-4.268016</td>\n",
              "      <td>-3.881110</td>\n",
              "      <td>-2.993280</td>\n",
              "      <td>-1.671131</td>\n",
              "      <td>-1.333884</td>\n",
              "      <td>-0.965629</td>\n",
              "      <td>-0.183319</td>\n",
              "      <td>-0.101657</td>\n",
              "      <td>-0.273874</td>\n",
              "      <td>-0.127818</td>\n",
              "      <td>-0.195983</td>\n",
              "      <td>-0.213523</td>\n",
              "      <td>-0.176473</td>\n",
              "      <td>-0.156932</td>\n",
              "      <td>-0.149172</td>\n",
              "      <td>-0.181510</td>\n",
              "      <td>-0.180074</td>\n",
              "      <td>-0.246151</td>\n",
              "      <td>-0.274260</td>\n",
              "      <td>-0.140960</td>\n",
              "      <td>-0.277449</td>\n",
              "      <td>-0.382549</td>\n",
              "      <td>-0.311937</td>\n",
              "      <td>-0.360093</td>\n",
              "      <td>-0.405968</td>\n",
              "      <td>-0.571433</td>\n",
              "      <td>-0.524106</td>\n",
              "      <td>-0.537886</td>\n",
              "      <td>-0.606778</td>\n",
              "      <td>-0.661446</td>\n",
              "      <td>-0.683375</td>\n",
              "      <td>-0.746683</td>\n",
              "      <td>-0.635662</td>\n",
              "      <td>-0.625231</td>\n",
              "      <td>-0.540094</td>\n",
              "      <td>-0.674995</td>\n",
              "      <td>...</td>\n",
              "      <td>1.772155</td>\n",
              "      <td>2.000769</td>\n",
              "      <td>1.925003</td>\n",
              "      <td>1.898426</td>\n",
              "      <td>1.720953</td>\n",
              "      <td>1.501711</td>\n",
              "      <td>1.422492</td>\n",
              "      <td>1.023225</td>\n",
              "      <td>0.776341</td>\n",
              "      <td>0.504426</td>\n",
              "      <td>0.056382</td>\n",
              "      <td>-0.233161</td>\n",
              "      <td>-0.406388</td>\n",
              "      <td>-0.327528</td>\n",
              "      <td>-0.460868</td>\n",
              "      <td>-0.402536</td>\n",
              "      <td>-0.345752</td>\n",
              "      <td>-0.354206</td>\n",
              "      <td>-0.439959</td>\n",
              "      <td>-0.425326</td>\n",
              "      <td>-0.439789</td>\n",
              "      <td>-0.451835</td>\n",
              "      <td>-0.395926</td>\n",
              "      <td>-0.448762</td>\n",
              "      <td>-0.391789</td>\n",
              "      <td>-0.376307</td>\n",
              "      <td>-0.461069</td>\n",
              "      <td>-0.253524</td>\n",
              "      <td>0.213006</td>\n",
              "      <td>0.491173</td>\n",
              "      <td>0.350816</td>\n",
              "      <td>0.499111</td>\n",
              "      <td>0.600345</td>\n",
              "      <td>0.842069</td>\n",
              "      <td>0.952074</td>\n",
              "      <td>0.990133</td>\n",
              "      <td>1.086798</td>\n",
              "      <td>1.403011</td>\n",
              "      <td>-0.383564</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.800232</td>\n",
              "      <td>-0.874252</td>\n",
              "      <td>-2.384761</td>\n",
              "      <td>-3.973292</td>\n",
              "      <td>-4.338224</td>\n",
              "      <td>-3.802422</td>\n",
              "      <td>-2.534510</td>\n",
              "      <td>-1.783423</td>\n",
              "      <td>-1.594450</td>\n",
              "      <td>-0.753199</td>\n",
              "      <td>-0.298107</td>\n",
              "      <td>-0.428928</td>\n",
              "      <td>-0.491351</td>\n",
              "      <td>-0.361304</td>\n",
              "      <td>-0.339296</td>\n",
              "      <td>-0.324952</td>\n",
              "      <td>-0.290113</td>\n",
              "      <td>-0.363051</td>\n",
              "      <td>-0.525684</td>\n",
              "      <td>-0.597423</td>\n",
              "      <td>-0.575523</td>\n",
              "      <td>-0.567503</td>\n",
              "      <td>-0.504555</td>\n",
              "      <td>-0.618406</td>\n",
              "      <td>-0.682814</td>\n",
              "      <td>-0.743849</td>\n",
              "      <td>-0.815588</td>\n",
              "      <td>-0.826902</td>\n",
              "      <td>-0.782374</td>\n",
              "      <td>-0.929462</td>\n",
              "      <td>-0.999672</td>\n",
              "      <td>-1.060969</td>\n",
              "      <td>-1.007877</td>\n",
              "      <td>-1.028735</td>\n",
              "      <td>-1.122629</td>\n",
              "      <td>-1.028650</td>\n",
              "      <td>-1.046515</td>\n",
              "      <td>-1.063372</td>\n",
              "      <td>-1.122423</td>\n",
              "      <td>-0.983242</td>\n",
              "      <td>...</td>\n",
              "      <td>1.155363</td>\n",
              "      <td>1.336254</td>\n",
              "      <td>1.627534</td>\n",
              "      <td>1.717594</td>\n",
              "      <td>1.696487</td>\n",
              "      <td>1.741686</td>\n",
              "      <td>1.674078</td>\n",
              "      <td>1.546928</td>\n",
              "      <td>1.331738</td>\n",
              "      <td>1.110168</td>\n",
              "      <td>0.922210</td>\n",
              "      <td>0.521777</td>\n",
              "      <td>0.154852</td>\n",
              "      <td>-0.123861</td>\n",
              "      <td>-0.202998</td>\n",
              "      <td>-0.247956</td>\n",
              "      <td>-0.219122</td>\n",
              "      <td>-0.214695</td>\n",
              "      <td>-0.319215</td>\n",
              "      <td>-0.198597</td>\n",
              "      <td>-0.151618</td>\n",
              "      <td>-0.129593</td>\n",
              "      <td>-0.074939</td>\n",
              "      <td>-0.196807</td>\n",
              "      <td>-0.174795</td>\n",
              "      <td>-0.208833</td>\n",
              "      <td>-0.210754</td>\n",
              "      <td>-0.100485</td>\n",
              "      <td>0.197446</td>\n",
              "      <td>0.966606</td>\n",
              "      <td>1.148884</td>\n",
              "      <td>0.958434</td>\n",
              "      <td>1.059025</td>\n",
              "      <td>1.371682</td>\n",
              "      <td>1.277392</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>0.971020</td>\n",
              "      <td>1.614392</td>\n",
              "      <td>1.421456</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3    ...       137       138       139  140\n",
              "0 -0.112522 -2.827204 -3.773897 -4.349751  ...  0.123431  0.925286  0.193137  1.0\n",
              "1 -1.100878 -3.996840 -4.285843 -4.506579  ...  0.773820  1.119621 -1.436250  1.0\n",
              "2 -0.567088 -2.593450 -3.874230 -4.584095  ...  0.321097  0.904227 -0.421797  1.0\n",
              "3  0.490473 -1.914407 -3.616364 -4.318823  ...  1.086798  1.403011 -0.383564  1.0\n",
              "4  0.800232 -0.874252 -2.384761 -3.973292  ...  0.971020  1.614392  1.421456  1.0\n",
              "\n",
              "[5 rows x 141 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "GxrU0pRathNu",
        "outputId": "2dd1a708-28ee-470d-9158-0a35b1e25998"
      },
      "source": [
        "# The last element contains the labels\r\n",
        "labels = raw_data[:, -1]\r\n",
        "\r\n",
        "# The other data points are the electrocadriogram data\r\n",
        "data = raw_data[:, 0:-1]\r\n",
        "\r\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=21)\r\n",
        "\r\n",
        "# Normalize to [0, 1]\r\n",
        "min_val = tf.reduce_min(train_data)\r\n",
        "max_val = tf.reduce_max(train_data)\r\n",
        "\r\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\r\n",
        "test_data = (test_data - min_val) / (max_val - min_val)\r\n",
        "\r\n",
        "train_data = tf.cast(train_data, tf.float32)\r\n",
        "test_data = tf.cast(test_data, tf.float32)\r\n",
        "\r\n",
        "# plot data\r\n",
        "plt.grid()\r\n",
        "plt.plot(np.arange(140), train_data[0])\r\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Oj3t5oR5tnQ3",
        "outputId": "8426d169-b121-42ae-a5b8-2b9440b4d650"
      },
      "source": [
        "plt.grid()\r\n",
        "plt.plot(np.arange(140), train_data[100])\r\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZmtTShZtobJ"
      },
      "source": [
        "All the above code for loading and preparing the data was provided by Richard Jiang."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKBDNmkTtrYN"
      },
      "source": [
        "# the model requires a 3-dimensional input therefore, we need to expand the \r\n",
        "# dimensions of the training and testing data.\r\n",
        "train_data = np.expand_dims(train_data, 2)\r\n",
        "test_data = np.expand_dims(test_data, 2)\r\n",
        "\r\n",
        "# the model requires these to be in binary matrix form.\r\n",
        "train_classes = to_categorical(train_labels, 2)\r\n",
        "test_classes = to_categorical(test_labels, 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHN2s5XQvi8l"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUu27IRuvgs1"
      },
      "source": [
        "#define the input shape for this data.\r\n",
        "input_shape = (140, 1)\r\n",
        "\r\n",
        "def buildmodel(epochs, batch_size):\r\n",
        "  #initiate a sequential keras model (sequential for simplicity)\r\n",
        "  model = Sequential()\r\n",
        "  #first, a LSTM layer\r\n",
        "  model.add(LSTM(100, input_shape=input_shape))\r\n",
        "  #dropout layer to reduce overfitting\r\n",
        "  model.add(Dropout(0.5)) \r\n",
        "  #fully-connected layer\r\n",
        "  model.add(Dense(100, activation='relu'))\r\n",
        "  #output layer\r\n",
        "  model.add(Dense(2, activation='softmax'))\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\t#fit network to the data\r\n",
        "  model.fit(train_data, train_classes, epochs=epochs, batch_size=batch_size,\r\n",
        "           shuffle=True, validation_data=(test_data, test_classes),\r\n",
        "           callbacks=[TensorBoard(log_dir='/tmp/model_cnn')])\r\n",
        "  return model\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RJJ7K8h0WUc"
      },
      "source": [
        "# Optimise Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2AUuo6i0aXj"
      },
      "source": [
        "Determine number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "PaJK8k7r0cw8",
        "outputId": "36486f84-1d0c-417c-a2e3-deecb0325501"
      },
      "source": [
        "#build the model \r\n",
        "model = buildmodel(15, 128)\r\n",
        "#plot the loss and the value loss over the different epochs.\r\n",
        "plt.plot(model.history.history[\"loss\"])\r\n",
        "plt.plot(model.history.history[\"val_loss\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "32/32 [==============================] - 12s 305ms/step - loss: 0.6414 - accuracy: 0.5768 - val_loss: 0.3878 - val_accuracy: 0.8870\n",
            "Epoch 2/15\n",
            "32/32 [==============================] - 10s 303ms/step - loss: 0.3488 - accuracy: 0.9078 - val_loss: 0.2844 - val_accuracy: 0.9510\n",
            "Epoch 3/15\n",
            "32/32 [==============================] - 9s 284ms/step - loss: 0.2512 - accuracy: 0.9520 - val_loss: 0.1258 - val_accuracy: 0.9580\n",
            "Epoch 4/15\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.1066 - accuracy: 0.9638 - val_loss: 0.0603 - val_accuracy: 0.9770\n",
            "Epoch 5/15\n",
            "32/32 [==============================] - 9s 292ms/step - loss: 0.0830 - accuracy: 0.9725 - val_loss: 0.0785 - val_accuracy: 0.9750\n",
            "Epoch 6/15\n",
            "32/32 [==============================] - 9s 295ms/step - loss: 0.0769 - accuracy: 0.9756 - val_loss: 0.0665 - val_accuracy: 0.9750\n",
            "Epoch 7/15\n",
            "32/32 [==============================] - 9s 292ms/step - loss: 0.1074 - accuracy: 0.9658 - val_loss: 0.0818 - val_accuracy: 0.9720\n",
            "Epoch 8/15\n",
            "32/32 [==============================] - 9s 281ms/step - loss: 0.0894 - accuracy: 0.9707 - val_loss: 0.0748 - val_accuracy: 0.9720\n",
            "Epoch 9/15\n",
            "32/32 [==============================] - 9s 283ms/step - loss: 0.0874 - accuracy: 0.9750 - val_loss: 0.0683 - val_accuracy: 0.9720\n",
            "Epoch 10/15\n",
            "32/32 [==============================] - 9s 283ms/step - loss: 0.0855 - accuracy: 0.9694 - val_loss: 0.1048 - val_accuracy: 0.9720\n",
            "Epoch 11/15\n",
            "32/32 [==============================] - 9s 283ms/step - loss: 0.0853 - accuracy: 0.9706 - val_loss: 0.0594 - val_accuracy: 0.9760\n",
            "Epoch 12/15\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.0929 - accuracy: 0.9738 - val_loss: 0.0777 - val_accuracy: 0.9740\n",
            "Epoch 13/15\n",
            "32/32 [==============================] - 9s 283ms/step - loss: 0.0799 - accuracy: 0.9758 - val_loss: 0.0677 - val_accuracy: 0.9780\n",
            "Epoch 14/15\n",
            "32/32 [==============================] - 9s 283ms/step - loss: 0.0739 - accuracy: 0.9794 - val_loss: 0.0515 - val_accuracy: 0.9810\n",
            "Epoch 15/15\n",
            "32/32 [==============================] - 9s 280ms/step - loss: 0.0964 - accuracy: 0.9693 - val_loss: 0.0662 - val_accuracy: 0.9790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb3867f8710>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKu494hN2BpS"
      },
      "source": [
        "It seems to stablise by 12 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQAmTUxV2jS1"
      },
      "source": [
        "Need to repeat optimisation experiments various times for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59eA237W2kcn"
      },
      "source": [
        "def buildmultiple(epochs, batch_size):\r\n",
        "  #initiate accuracy list.\r\n",
        "  repeat_accuracy = []*10\r\n",
        "  #for each repeat\r\n",
        "  for i in range(10):\r\n",
        "    #build the model\r\n",
        "    model = buildmodel(epochs, batch_size)\r\n",
        "    #measure and record the accuracy\r\n",
        "    loss, accuracy = model.evaluate(test_data, test_classes, \r\n",
        "                                          batch_size=batch_size)\r\n",
        "    repeat_accuracy.append(accuracy)\r\n",
        "  return repeat_accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FvF0FK92HTq"
      },
      "source": [
        "Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ewsc-Qrd2YHn",
        "outputId": "a1833461-e0a4-4edd-9ae1-debb8eb99457"
      },
      "source": [
        "#list of batch sizes to be tested.\r\n",
        "batch = [8, 16, 32, 64, 128, 256]\r\n",
        "\r\n",
        "#initiate accuracy list.\r\n",
        "batch_accuracy = []*6\r\n",
        "\r\n",
        "#repeat following steps for each batch size.\r\n",
        "for i in range(len(batch)):\r\n",
        "  accuracy = buildmultiple(12, batch[i])\r\n",
        "  batch_accuracy.append(accuracy)  \r\n",
        "\r\n",
        "#boxplot of the accuracy scores against the corresponding batch sizes \r\n",
        "#investigated. \r\n",
        "plt.boxplot(batch_accuracy, labels = batch)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "500/500 [==============================] - 37s 70ms/step - loss: 0.4697 - accuracy: 0.7918 - val_loss: 0.2472 - val_accuracy: 0.9260\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.1463 - accuracy: 0.9547 - val_loss: 0.2889 - val_accuracy: 0.8900\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.2185 - accuracy: 0.9292 - val_loss: 0.2333 - val_accuracy: 0.9100\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.1878 - accuracy: 0.9399 - val_loss: 0.1059 - val_accuracy: 0.9540\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1343 - accuracy: 0.9583 - val_loss: 0.1628 - val_accuracy: 0.9460\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.1296 - accuracy: 0.9575 - val_loss: 0.0902 - val_accuracy: 0.9600\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1475 - accuracy: 0.9491 - val_loss: 0.1336 - val_accuracy: 0.9470\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0977 - accuracy: 0.9686 - val_loss: 0.0421 - val_accuracy: 0.9840\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0648 - accuracy: 0.9798 - val_loss: 0.0532 - val_accuracy: 0.9820\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0710 - accuracy: 0.9793 - val_loss: 0.0585 - val_accuracy: 0.9810\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0704 - accuracy: 0.9766 - val_loss: 0.0489 - val_accuracy: 0.9810\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 34s 69ms/step - loss: 0.0550 - accuracy: 0.9849 - val_loss: 0.1518 - val_accuracy: 0.9460\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.1518 - accuracy: 0.9460\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 36s 68ms/step - loss: 0.4466 - accuracy: 0.7898 - val_loss: 0.2071 - val_accuracy: 0.9420\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1562 - accuracy: 0.9507 - val_loss: 0.1966 - val_accuracy: 0.9510\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.1457 - accuracy: 0.9559 - val_loss: 0.0774 - val_accuracy: 0.9800\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0990 - accuracy: 0.9690 - val_loss: 0.0671 - val_accuracy: 0.9810\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0920 - accuracy: 0.9741 - val_loss: 0.0486 - val_accuracy: 0.9840\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0735 - accuracy: 0.9794 - val_loss: 0.0476 - val_accuracy: 0.9870\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.1198 - val_accuracy: 0.9560\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0878 - accuracy: 0.9723 - val_loss: 0.0404 - val_accuracy: 0.9860\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0663 - accuracy: 0.9800 - val_loss: 0.0502 - val_accuracy: 0.9840\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 34s 69ms/step - loss: 0.0615 - accuracy: 0.9830 - val_loss: 0.0504 - val_accuracy: 0.9840\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0842 - accuracy: 0.9747 - val_loss: 0.0433 - val_accuracy: 0.9860\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0655 - accuracy: 0.9801 - val_loss: 0.0518 - val_accuracy: 0.9850\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0518 - accuracy: 0.9850\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 36s 68ms/step - loss: 0.3617 - accuracy: 0.8422 - val_loss: 0.1630 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.1458 - accuracy: 0.9576 - val_loss: 0.1884 - val_accuracy: 0.9540\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1452 - accuracy: 0.9553 - val_loss: 0.0711 - val_accuracy: 0.9780\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0977 - accuracy: 0.9716 - val_loss: 0.1549 - val_accuracy: 0.9580\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0983 - accuracy: 0.9687 - val_loss: 0.0609 - val_accuracy: 0.9820\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0897 - accuracy: 0.9724 - val_loss: 0.0601 - val_accuracy: 0.9780\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1087 - accuracy: 0.9642 - val_loss: 0.0495 - val_accuracy: 0.9820\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1171 - accuracy: 0.9537 - val_loss: 0.1499 - val_accuracy: 0.9450\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1036 - accuracy: 0.9653 - val_loss: 0.0511 - val_accuracy: 0.9820\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1084 - accuracy: 0.9646 - val_loss: 0.0596 - val_accuracy: 0.9800\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0663 - accuracy: 0.9800 - val_loss: 0.0532 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0614 - accuracy: 0.9798 - val_loss: 0.0360 - val_accuracy: 0.9910\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0360 - accuracy: 0.9910\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 35s 66ms/step - loss: 0.4017 - accuracy: 0.8265 - val_loss: 0.2695 - val_accuracy: 0.9410\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.2263 - accuracy: 0.9284 - val_loss: 0.1489 - val_accuracy: 0.9520\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1405 - accuracy: 0.9525 - val_loss: 0.0599 - val_accuracy: 0.9800\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0753 - accuracy: 0.9761 - val_loss: 0.0625 - val_accuracy: 0.9780\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1286 - accuracy: 0.9616 - val_loss: 0.0700 - val_accuracy: 0.9760\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0742 - accuracy: 0.9782 - val_loss: 0.0427 - val_accuracy: 0.9860\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0538 - accuracy: 0.9848 - val_loss: 0.0411 - val_accuracy: 0.9860\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0605 - accuracy: 0.9827 - val_loss: 0.0390 - val_accuracy: 0.9860\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 34s 69ms/step - loss: 0.3230 - accuracy: 0.8866 - val_loss: 0.1634 - val_accuracy: 0.9480\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 35s 69ms/step - loss: 0.0943 - accuracy: 0.9730 - val_loss: 0.0720 - val_accuracy: 0.9720\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0710 - accuracy: 0.9814 - val_loss: 0.0422 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.0645 - accuracy: 0.9802 - val_loss: 0.0437 - val_accuracy: 0.9860\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0437 - accuracy: 0.9860\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 37s 69ms/step - loss: 0.4524 - accuracy: 0.7814 - val_loss: 0.0959 - val_accuracy: 0.9640\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1058 - accuracy: 0.9640 - val_loss: 0.0987 - val_accuracy: 0.9700\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1381 - accuracy: 0.9631 - val_loss: 0.6862 - val_accuracy: 0.5600\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.6827 - accuracy: 0.5904 - val_loss: 0.6869 - val_accuracy: 0.5600\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.6791 - accuracy: 0.5918 - val_loss: 0.6909 - val_accuracy: 0.5600\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.6808 - accuracy: 0.5809 - val_loss: 0.6921 - val_accuracy: 0.5600\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.6746 - accuracy: 0.5923 - val_loss: 0.7022 - val_accuracy: 0.5600\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.6699 - accuracy: 0.5876 - val_loss: 0.5277 - val_accuracy: 0.9100\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.4711 - accuracy: 0.7927 - val_loss: 0.1536 - val_accuracy: 0.9550\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 36s 73ms/step - loss: 0.1777 - accuracy: 0.9440 - val_loss: 0.1387 - val_accuracy: 0.9570\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1549 - accuracy: 0.9507 - val_loss: 0.1106 - val_accuracy: 0.9590\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1331 - accuracy: 0.9589 - val_loss: 0.0730 - val_accuracy: 0.9790\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0730 - accuracy: 0.9790\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 37s 70ms/step - loss: 0.4695 - accuracy: 0.7991 - val_loss: 0.1313 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1559 - accuracy: 0.9499 - val_loss: 0.2645 - val_accuracy: 0.9270\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1937 - accuracy: 0.9447 - val_loss: 0.1750 - val_accuracy: 0.9480\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.1623 - accuracy: 0.9496 - val_loss: 0.2058 - val_accuracy: 0.9230\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1584 - accuracy: 0.9489 - val_loss: 0.0596 - val_accuracy: 0.9820\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0857 - accuracy: 0.9748 - val_loss: 0.0617 - val_accuracy: 0.9870\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 36s 73ms/step - loss: 0.0922 - accuracy: 0.9717 - val_loss: 0.1895 - val_accuracy: 0.9390\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 35s 70ms/step - loss: 0.1870 - accuracy: 0.9400 - val_loss: 0.0906 - val_accuracy: 0.9600\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1136 - accuracy: 0.9591 - val_loss: 0.1161 - val_accuracy: 0.9560\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0830 - accuracy: 0.9757 - val_loss: 0.0382 - val_accuracy: 0.9880\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 36s 73ms/step - loss: 0.0583 - accuracy: 0.9817 - val_loss: 0.2783 - val_accuracy: 0.9290\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1085 - accuracy: 0.9717 - val_loss: 0.0520 - val_accuracy: 0.9830\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0520 - accuracy: 0.9830\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 37s 70ms/step - loss: 0.4230 - accuracy: 0.8155 - val_loss: 0.0842 - val_accuracy: 0.9670\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1489 - accuracy: 0.9583 - val_loss: 0.0810 - val_accuracy: 0.9780\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1194 - accuracy: 0.9601 - val_loss: 0.0773 - val_accuracy: 0.9730\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0901 - accuracy: 0.9716 - val_loss: 0.0653 - val_accuracy: 0.9790\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0792 - accuracy: 0.9759 - val_loss: 0.0530 - val_accuracy: 0.9840\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0769 - accuracy: 0.9750 - val_loss: 0.1625 - val_accuracy: 0.9550\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1338 - accuracy: 0.9617 - val_loss: 0.0812 - val_accuracy: 0.9760\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0770 - accuracy: 0.9774 - val_loss: 0.0453 - val_accuracy: 0.9820\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0525 - accuracy: 0.9850 - val_loss: 0.0705 - val_accuracy: 0.9780\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.0850 - accuracy: 0.9790 - val_loss: 0.0532 - val_accuracy: 0.9830\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 35s 71ms/step - loss: 0.1794 - accuracy: 0.9434 - val_loss: 0.1375 - val_accuracy: 0.9520\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 37s 73ms/step - loss: 0.0955 - accuracy: 0.9712 - val_loss: 0.0684 - val_accuracy: 0.9780\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0684 - accuracy: 0.9780\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 38s 72ms/step - loss: 0.7132 - accuracy: 0.6421 - val_loss: 0.1435 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1804 - accuracy: 0.9453 - val_loss: 0.3929 - val_accuracy: 0.8450\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.1280 - accuracy: 0.9600 - val_loss: 0.2502 - val_accuracy: 0.9380\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.2184 - accuracy: 0.9321 - val_loss: 0.1462 - val_accuracy: 0.9560\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 36s 73ms/step - loss: 0.1797 - accuracy: 0.9466 - val_loss: 0.1443 - val_accuracy: 0.9530\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 37s 73ms/step - loss: 0.1446 - accuracy: 0.9529 - val_loss: 0.0575 - val_accuracy: 0.9790\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0876 - accuracy: 0.9725 - val_loss: 0.0701 - val_accuracy: 0.9760\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 38s 75ms/step - loss: 0.0752 - accuracy: 0.9797 - val_loss: 0.0614 - val_accuracy: 0.9780\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.0840 - accuracy: 0.9732 - val_loss: 0.1160 - val_accuracy: 0.9550\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0823 - accuracy: 0.9719 - val_loss: 0.0514 - val_accuracy: 0.9850\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0724 - accuracy: 0.9803 - val_loss: 0.0401 - val_accuracy: 0.9870\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0608 - accuracy: 0.9796 - val_loss: 0.0608 - val_accuracy: 0.9830\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0608 - accuracy: 0.9830\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 37s 71ms/step - loss: 0.4386 - accuracy: 0.8040 - val_loss: 0.1136 - val_accuracy: 0.9600\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 37s 73ms/step - loss: 0.1539 - accuracy: 0.9497 - val_loss: 0.0718 - val_accuracy: 0.9750\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.1118 - accuracy: 0.9659 - val_loss: 0.0455 - val_accuracy: 0.9850\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0807 - accuracy: 0.9763 - val_loss: 0.0494 - val_accuracy: 0.9860\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0967 - accuracy: 0.9733 - val_loss: 0.1044 - val_accuracy: 0.9710\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0796 - accuracy: 0.9769 - val_loss: 0.0502 - val_accuracy: 0.9840\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0747 - accuracy: 0.9790 - val_loss: 0.0540 - val_accuracy: 0.9850\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0600 - accuracy: 0.9815 - val_loss: 0.0343 - val_accuracy: 0.9910\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0624 - accuracy: 0.9818 - val_loss: 0.0490 - val_accuracy: 0.9840\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 36s 71ms/step - loss: 0.0784 - accuracy: 0.9777 - val_loss: 0.0365 - val_accuracy: 0.9930\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0759 - accuracy: 0.9790 - val_loss: 0.0503 - val_accuracy: 0.9770\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 36s 72ms/step - loss: 0.0649 - accuracy: 0.9800 - val_loss: 0.0462 - val_accuracy: 0.9840\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0462 - accuracy: 0.9840\n",
            "Epoch 1/12\n",
            "500/500 [==============================] - 38s 73ms/step - loss: 0.4170 - accuracy: 0.8033 - val_loss: 0.1350 - val_accuracy: 0.9620\n",
            "Epoch 2/12\n",
            "500/500 [==============================] - 36s 73ms/step - loss: 0.1367 - accuracy: 0.9543 - val_loss: 0.1249 - val_accuracy: 0.9580\n",
            "Epoch 3/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.1775 - accuracy: 0.9446 - val_loss: 0.1402 - val_accuracy: 0.9440\n",
            "Epoch 4/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.1218 - accuracy: 0.9611 - val_loss: 0.0636 - val_accuracy: 0.9770\n",
            "Epoch 5/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.0916 - accuracy: 0.9701 - val_loss: 0.0718 - val_accuracy: 0.9760\n",
            "Epoch 6/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.1210 - accuracy: 0.9610 - val_loss: 0.0457 - val_accuracy: 0.9850\n",
            "Epoch 7/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.0552 - accuracy: 0.9834 - val_loss: 0.0742 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "500/500 [==============================] - 38s 76ms/step - loss: 0.1713 - accuracy: 0.9539 - val_loss: 0.1483 - val_accuracy: 0.9380\n",
            "Epoch 9/12\n",
            "500/500 [==============================] - 38s 76ms/step - loss: 0.1061 - accuracy: 0.9626 - val_loss: 0.0414 - val_accuracy: 0.9870\n",
            "Epoch 10/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 0.0373 - val_accuracy: 0.9890\n",
            "Epoch 11/12\n",
            "500/500 [==============================] - 37s 75ms/step - loss: 0.0856 - accuracy: 0.9715 - val_loss: 0.0336 - val_accuracy: 0.9900\n",
            "Epoch 12/12\n",
            "500/500 [==============================] - 37s 74ms/step - loss: 0.0741 - accuracy: 0.9753 - val_loss: 0.0483 - val_accuracy: 0.9850\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.0483 - accuracy: 0.9850\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 80ms/step - loss: 0.4233 - accuracy: 0.7720 - val_loss: 0.2413 - val_accuracy: 0.9520\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 0.1670 - accuracy: 0.9529 - val_loss: 0.1562 - val_accuracy: 0.9460\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 0.1409 - accuracy: 0.9585 - val_loss: 0.1320 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1086 - accuracy: 0.9625 - val_loss: 0.0662 - val_accuracy: 0.9780\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.0877 - accuracy: 0.9720 - val_loss: 0.0655 - val_accuracy: 0.9790\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 20s 78ms/step - loss: 0.0958 - accuracy: 0.9657 - val_loss: 0.0505 - val_accuracy: 0.9860\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.0580 - accuracy: 0.9812 - val_loss: 0.0625 - val_accuracy: 0.9790\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0631 - accuracy: 0.9822 - val_loss: 0.0484 - val_accuracy: 0.9860\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0647 - accuracy: 0.9820 - val_loss: 0.2420 - val_accuracy: 0.9390\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 22s 87ms/step - loss: 0.1794 - accuracy: 0.9395 - val_loss: 0.1160 - val_accuracy: 0.9610\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1328 - accuracy: 0.9578 - val_loss: 0.1717 - val_accuracy: 0.9510\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.1700 - accuracy: 0.9471 - val_loss: 0.0907 - val_accuracy: 0.9580\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0907 - accuracy: 0.9580\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 79ms/step - loss: 0.4507 - accuracy: 0.7792 - val_loss: 0.1171 - val_accuracy: 0.9620\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.1358 - accuracy: 0.9543 - val_loss: 0.2787 - val_accuracy: 0.8720\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1794 - accuracy: 0.9419 - val_loss: 0.0687 - val_accuracy: 0.9750\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1353 - accuracy: 0.9577 - val_loss: 0.0641 - val_accuracy: 0.9830\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1149 - accuracy: 0.9648 - val_loss: 0.1061 - val_accuracy: 0.9710\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1077 - accuracy: 0.9683 - val_loss: 0.0607 - val_accuracy: 0.9800\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0761 - accuracy: 0.9761 - val_loss: 0.0475 - val_accuracy: 0.9850\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0654 - accuracy: 0.9802 - val_loss: 0.0480 - val_accuracy: 0.9820\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.0638 - val_accuracy: 0.9780\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0737 - accuracy: 0.9782 - val_loss: 0.0610 - val_accuracy: 0.9830\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 86ms/step - loss: 0.0719 - accuracy: 0.9794 - val_loss: 0.0577 - val_accuracy: 0.9810\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 86ms/step - loss: 0.0557 - accuracy: 0.9819 - val_loss: 0.0369 - val_accuracy: 0.9870\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0369 - accuracy: 0.9870\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 21s 79ms/step - loss: 0.4612 - accuracy: 0.7895 - val_loss: 0.0821 - val_accuracy: 0.9690\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.2790 - accuracy: 0.9114 - val_loss: 0.1502 - val_accuracy: 0.9580\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1497 - accuracy: 0.9523 - val_loss: 0.0771 - val_accuracy: 0.9710\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 20s 80ms/step - loss: 0.1154 - accuracy: 0.9644 - val_loss: 0.1016 - val_accuracy: 0.9660\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1301 - accuracy: 0.9579 - val_loss: 0.0790 - val_accuracy: 0.9660\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 20s 80ms/step - loss: 0.0945 - accuracy: 0.9709 - val_loss: 0.0533 - val_accuracy: 0.9840\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1343 - accuracy: 0.9550 - val_loss: 0.0701 - val_accuracy: 0.9780\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0882 - accuracy: 0.9743 - val_loss: 0.0730 - val_accuracy: 0.9780\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0636 - accuracy: 0.9798 - val_loss: 0.0519 - val_accuracy: 0.9830\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0978 - accuracy: 0.9682 - val_loss: 0.0558 - val_accuracy: 0.9820\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0841 - accuracy: 0.9755 - val_loss: 0.0386 - val_accuracy: 0.9870\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0636 - accuracy: 0.9831 - val_loss: 0.0386 - val_accuracy: 0.9890\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0386 - accuracy: 0.9890\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 21s 79ms/step - loss: 0.4707 - accuracy: 0.7748 - val_loss: 0.1144 - val_accuracy: 0.9590\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.1590 - accuracy: 0.9513 - val_loss: 0.1293 - val_accuracy: 0.9510\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 22s 87ms/step - loss: 0.1773 - accuracy: 0.9436 - val_loss: 0.1561 - val_accuracy: 0.9570\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1478 - accuracy: 0.9558 - val_loss: 0.0801 - val_accuracy: 0.9620\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 20s 80ms/step - loss: 0.0996 - accuracy: 0.9649 - val_loss: 0.0626 - val_accuracy: 0.9790\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0868 - accuracy: 0.9708 - val_loss: 0.3709 - val_accuracy: 0.8420\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.2934 - accuracy: 0.8981 - val_loss: 0.1328 - val_accuracy: 0.9560\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1464 - accuracy: 0.9560 - val_loss: 0.0906 - val_accuracy: 0.9660\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.2231 - accuracy: 0.9272 - val_loss: 0.1417 - val_accuracy: 0.9540\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1315 - accuracy: 0.9572 - val_loss: 0.0720 - val_accuracy: 0.9750\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0783 - accuracy: 0.9780 - val_loss: 1.2347 - val_accuracy: 0.7920\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.5214 - accuracy: 0.8910 - val_loss: 0.1814 - val_accuracy: 0.9500\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.1814 - accuracy: 0.9500\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 80ms/step - loss: 0.4810 - accuracy: 0.7498 - val_loss: 0.1666 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.1454 - accuracy: 0.9551 - val_loss: 0.1546 - val_accuracy: 0.9600\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1160 - accuracy: 0.9594 - val_loss: 0.0914 - val_accuracy: 0.9680\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1004 - accuracy: 0.9668 - val_loss: 0.0908 - val_accuracy: 0.9600\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0788 - accuracy: 0.9791 - val_loss: 0.0527 - val_accuracy: 0.9810\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 0.0889 - accuracy: 0.9755 - val_loss: 0.0466 - val_accuracy: 0.9840\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0638 - accuracy: 0.9800 - val_loss: 0.0740 - val_accuracy: 0.9750\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0854 - accuracy: 0.9789 - val_loss: 0.1538 - val_accuracy: 0.9550\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1167 - accuracy: 0.9639 - val_loss: 0.0475 - val_accuracy: 0.9850\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0653 - accuracy: 0.9804 - val_loss: 0.0359 - val_accuracy: 0.9890\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0652 - accuracy: 0.9808 - val_loss: 0.1771 - val_accuracy: 0.9490\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.1099 - accuracy: 0.9677 - val_loss: 0.0499 - val_accuracy: 0.9830\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0499 - accuracy: 0.9830\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 82ms/step - loss: 0.4752 - accuracy: 0.7677 - val_loss: 0.1616 - val_accuracy: 0.9560\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.3022 - accuracy: 0.9050 - val_loss: 0.2047 - val_accuracy: 0.9440\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.2111 - accuracy: 0.9411 - val_loss: 0.1756 - val_accuracy: 0.9490\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 0.0859 - val_accuracy: 0.9630\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0872 - accuracy: 0.9747 - val_loss: 0.0467 - val_accuracy: 0.9850\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0756 - accuracy: 0.9776 - val_loss: 0.0469 - val_accuracy: 0.9820\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0630 - accuracy: 0.9806 - val_loss: 0.0474 - val_accuracy: 0.9840\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0707 - accuracy: 0.9774 - val_loss: 0.0512 - val_accuracy: 0.9850\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 22s 87ms/step - loss: 0.0580 - accuracy: 0.9841 - val_loss: 0.0465 - val_accuracy: 0.9830\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0542 - accuracy: 0.9830 - val_loss: 0.0470 - val_accuracy: 0.9870\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0573 - accuracy: 0.9820 - val_loss: 0.0434 - val_accuracy: 0.9860\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0722 - accuracy: 0.9796 - val_loss: 0.0474 - val_accuracy: 0.9850\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.0474 - accuracy: 0.9850\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 79ms/step - loss: 0.4930 - accuracy: 0.7467 - val_loss: 0.1003 - val_accuracy: 0.9680\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.1414 - accuracy: 0.9518 - val_loss: 0.1685 - val_accuracy: 0.9550\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1486 - accuracy: 0.9512 - val_loss: 0.1826 - val_accuracy: 0.9530\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1351 - accuracy: 0.9570 - val_loss: 0.0615 - val_accuracy: 0.9780\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1094 - accuracy: 0.9650 - val_loss: 0.0634 - val_accuracy: 0.9770\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0767 - accuracy: 0.9753 - val_loss: 0.0648 - val_accuracy: 0.9770\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1005 - accuracy: 0.9634 - val_loss: 0.0501 - val_accuracy: 0.9820\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0688 - accuracy: 0.9770 - val_loss: 0.0451 - val_accuracy: 0.9830\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0658 - accuracy: 0.9827 - val_loss: 0.0417 - val_accuracy: 0.9850\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0475 - accuracy: 0.9858 - val_loss: 0.0335 - val_accuracy: 0.9920\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.0759 - accuracy: 0.9796 - val_loss: 0.0515 - val_accuracy: 0.9890\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.0649 - accuracy: 0.9820 - val_loss: 0.0347 - val_accuracy: 0.9890\n",
            "63/63 [==============================] - 1s 21ms/step - loss: 0.0347 - accuracy: 0.9890\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 23s 83ms/step - loss: 0.4464 - accuracy: 0.7871 - val_loss: 0.1553 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.2014 - accuracy: 0.9437 - val_loss: 0.1596 - val_accuracy: 0.9550\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1408 - accuracy: 0.9562 - val_loss: 0.1041 - val_accuracy: 0.9690\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 20s 79ms/step - loss: 0.1477 - accuracy: 0.9518 - val_loss: 0.1829 - val_accuracy: 0.9540\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.1082 - accuracy: 0.9693 - val_loss: 0.0563 - val_accuracy: 0.9810\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0852 - accuracy: 0.9737 - val_loss: 0.0451 - val_accuracy: 0.9830\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0688 - accuracy: 0.9796 - val_loss: 0.0400 - val_accuracy: 0.9900\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0834 - accuracy: 0.9772 - val_loss: 0.0500 - val_accuracy: 0.9850\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0779 - accuracy: 0.9757 - val_loss: 0.0364 - val_accuracy: 0.9890\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0753 - accuracy: 0.9785 - val_loss: 0.0378 - val_accuracy: 0.9870\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.0639 - accuracy: 0.9812 - val_loss: 0.0411 - val_accuracy: 0.9890\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.1307 - accuracy: 0.9546 - val_loss: 0.1794 - val_accuracy: 0.9430\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.1794 - accuracy: 0.9430\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 83ms/step - loss: 0.4492 - accuracy: 0.7589 - val_loss: 0.0838 - val_accuracy: 0.9700\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 21s 86ms/step - loss: 0.1247 - accuracy: 0.9610 - val_loss: 0.1201 - val_accuracy: 0.9600\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.0523 - val_accuracy: 0.9800\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.1170 - accuracy: 0.9609 - val_loss: 0.1136 - val_accuracy: 0.9620\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0881 - accuracy: 0.9733 - val_loss: 0.0432 - val_accuracy: 0.9850\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0686 - accuracy: 0.9794 - val_loss: 0.0572 - val_accuracy: 0.9800\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0628 - accuracy: 0.9827 - val_loss: 0.0509 - val_accuracy: 0.9830\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0875 - accuracy: 0.9720 - val_loss: 0.1544 - val_accuracy: 0.9510\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.1430 - accuracy: 0.9484 - val_loss: 0.0458 - val_accuracy: 0.9910\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 22s 87ms/step - loss: 0.0854 - accuracy: 0.9696 - val_loss: 0.0540 - val_accuracy: 0.9840\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0650 - accuracy: 0.9786 - val_loss: 0.0414 - val_accuracy: 0.9870\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0586 - accuracy: 0.9816 - val_loss: 0.0599 - val_accuracy: 0.9800\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0599 - accuracy: 0.9800\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 22s 83ms/step - loss: 0.6058 - accuracy: 0.6612 - val_loss: 0.3491 - val_accuracy: 0.8670\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.2602 - accuracy: 0.9043 - val_loss: 0.1303 - val_accuracy: 0.9570\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.1188 - accuracy: 0.9648 - val_loss: 0.1203 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.1218 - accuracy: 0.9601 - val_loss: 0.1423 - val_accuracy: 0.9620\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.1467 - accuracy: 0.9534 - val_loss: 0.0887 - val_accuracy: 0.9660\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.1168 - accuracy: 0.9634 - val_loss: 0.0610 - val_accuracy: 0.9820\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.0778 - accuracy: 0.9756 - val_loss: 0.0558 - val_accuracy: 0.9800\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0722 - accuracy: 0.9780 - val_loss: 0.0388 - val_accuracy: 0.9880\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0666 - accuracy: 0.9809 - val_loss: 0.0363 - val_accuracy: 0.9900\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 22s 86ms/step - loss: 0.0802 - accuracy: 0.9755 - val_loss: 0.0406 - val_accuracy: 0.9870\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0599 - accuracy: 0.9861 - val_loss: 0.0450 - val_accuracy: 0.9900\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 21s 85ms/step - loss: 0.0592 - accuracy: 0.9821 - val_loss: 0.0462 - val_accuracy: 0.9850\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.0462 - accuracy: 0.9850\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 106ms/step - loss: 0.5492 - accuracy: 0.6873 - val_loss: 0.1728 - val_accuracy: 0.9600\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.1405 - accuracy: 0.9584 - val_loss: 0.0888 - val_accuracy: 0.9680\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1072 - accuracy: 0.9616 - val_loss: 0.1079 - val_accuracy: 0.9640\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.1167 - accuracy: 0.9582 - val_loss: 0.0721 - val_accuracy: 0.9760\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.1069 - accuracy: 0.9624 - val_loss: 0.1143 - val_accuracy: 0.9590\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.1187 - accuracy: 0.9614 - val_loss: 0.0599 - val_accuracy: 0.9780\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0709 - accuracy: 0.9795 - val_loss: 0.0826 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0788 - accuracy: 0.9729 - val_loss: 0.0496 - val_accuracy: 0.9850\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0749 - accuracy: 0.9788 - val_loss: 0.1407 - val_accuracy: 0.9570\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.1080 - accuracy: 0.9594 - val_loss: 0.0484 - val_accuracy: 0.9820\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.0517 - val_accuracy: 0.9880\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0598 - accuracy: 0.9829 - val_loss: 0.0384 - val_accuracy: 0.9890\n",
            "32/32 [==============================] - 1s 34ms/step - loss: 0.0384 - accuracy: 0.9890\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 108ms/step - loss: 0.6777 - accuracy: 0.6571 - val_loss: 0.1876 - val_accuracy: 0.9590\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1695 - accuracy: 0.9534 - val_loss: 0.0815 - val_accuracy: 0.9700\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0961 - accuracy: 0.9701 - val_loss: 0.1270 - val_accuracy: 0.9570\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.1185 - accuracy: 0.9613 - val_loss: 0.0687 - val_accuracy: 0.9750\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1209 - accuracy: 0.9602 - val_loss: 0.0697 - val_accuracy: 0.9750\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0953 - accuracy: 0.9669 - val_loss: 0.0702 - val_accuracy: 0.9760\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0873 - accuracy: 0.9730 - val_loss: 0.0811 - val_accuracy: 0.9650\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0968 - accuracy: 0.9673 - val_loss: 0.0856 - val_accuracy: 0.9590\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0874 - accuracy: 0.9718 - val_loss: 0.0735 - val_accuracy: 0.9720\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0837 - accuracy: 0.9711 - val_loss: 0.0517 - val_accuracy: 0.9800\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0873 - accuracy: 0.9725 - val_loss: 0.0615 - val_accuracy: 0.9780\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1020 - accuracy: 0.9671 - val_loss: 0.1083 - val_accuracy: 0.9570\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.1083 - accuracy: 0.9570\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 105ms/step - loss: 0.5225 - accuracy: 0.7226 - val_loss: 0.2119 - val_accuracy: 0.9490\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.1762 - accuracy: 0.9496 - val_loss: 0.0704 - val_accuracy: 0.9720\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1247 - accuracy: 0.9638 - val_loss: 0.0975 - val_accuracy: 0.9670\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0904 - accuracy: 0.9721 - val_loss: 0.0780 - val_accuracy: 0.9750\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0935 - accuracy: 0.9745 - val_loss: 0.0830 - val_accuracy: 0.9730\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0951 - accuracy: 0.9676 - val_loss: 0.0622 - val_accuracy: 0.9800\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0941 - accuracy: 0.9755 - val_loss: 0.0923 - val_accuracy: 0.9680\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1042 - accuracy: 0.9673 - val_loss: 0.0529 - val_accuracy: 0.9820\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0799 - accuracy: 0.9763 - val_loss: 0.0754 - val_accuracy: 0.9760\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0581 - accuracy: 0.9814 - val_loss: 0.0476 - val_accuracy: 0.9880\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0657 - accuracy: 0.9821 - val_loss: 0.0426 - val_accuracy: 0.9930\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0574 - accuracy: 0.9836 - val_loss: 0.0515 - val_accuracy: 0.9810\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0515 - accuracy: 0.9810\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 106ms/step - loss: 0.5264 - accuracy: 0.6996 - val_loss: 0.1447 - val_accuracy: 0.9640\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1343 - accuracy: 0.9651 - val_loss: 0.0707 - val_accuracy: 0.9760\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1149 - accuracy: 0.9602 - val_loss: 0.0702 - val_accuracy: 0.9690\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.1461 - accuracy: 0.9578 - val_loss: 0.0617 - val_accuracy: 0.9830\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.0902 - accuracy: 0.9756 - val_loss: 0.0688 - val_accuracy: 0.9760\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.0482 - val_accuracy: 0.9870\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1369 - accuracy: 0.9630 - val_loss: 0.1594 - val_accuracy: 0.9560\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1461 - accuracy: 0.9566 - val_loss: 0.1209 - val_accuracy: 0.9610\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1809 - accuracy: 0.9435 - val_loss: 0.1207 - val_accuracy: 0.9620\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1254 - accuracy: 0.9578 - val_loss: 0.0819 - val_accuracy: 0.9660\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.1138 - accuracy: 0.9633 - val_loss: 0.0864 - val_accuracy: 0.9630\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1037 - accuracy: 0.9654 - val_loss: 0.0824 - val_accuracy: 0.9650\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0824 - accuracy: 0.9650\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 108ms/step - loss: 0.5400 - accuracy: 0.6986 - val_loss: 0.3314 - val_accuracy: 0.9150\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.2551 - accuracy: 0.9315 - val_loss: 0.1041 - val_accuracy: 0.9640\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1262 - accuracy: 0.9608 - val_loss: 0.1575 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1540 - accuracy: 0.9495 - val_loss: 0.0821 - val_accuracy: 0.9690\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1131 - accuracy: 0.9649 - val_loss: 0.1078 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1009 - accuracy: 0.9652 - val_loss: 0.0850 - val_accuracy: 0.9650\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0908 - accuracy: 0.9704 - val_loss: 0.0798 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0748 - accuracy: 0.9790 - val_loss: 0.0560 - val_accuracy: 0.9840\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0619 - accuracy: 0.9805 - val_loss: 0.0581 - val_accuracy: 0.9820\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0636 - accuracy: 0.9848 - val_loss: 0.1274 - val_accuracy: 0.9590\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0927 - accuracy: 0.9741 - val_loss: 0.0446 - val_accuracy: 0.9840\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0680 - accuracy: 0.9768 - val_loss: 0.0603 - val_accuracy: 0.9830\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.0603 - accuracy: 0.9830\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 107ms/step - loss: 0.5345 - accuracy: 0.7145 - val_loss: 0.1163 - val_accuracy: 0.9630\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1137 - accuracy: 0.9606 - val_loss: 0.0941 - val_accuracy: 0.9660\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1094 - accuracy: 0.9673 - val_loss: 0.0949 - val_accuracy: 0.9620\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1042 - accuracy: 0.9697 - val_loss: 0.1033 - val_accuracy: 0.9630\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.1447 - accuracy: 0.9582 - val_loss: 0.1268 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.1167 - accuracy: 0.9643 - val_loss: 0.1172 - val_accuracy: 0.9600\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1024 - accuracy: 0.9685 - val_loss: 0.0484 - val_accuracy: 0.9830\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0873 - accuracy: 0.9729 - val_loss: 0.0474 - val_accuracy: 0.9830\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 0.0584 - val_accuracy: 0.9830\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0533 - accuracy: 0.9847 - val_loss: 0.0436 - val_accuracy: 0.9860\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0635 - accuracy: 0.9811 - val_loss: 0.0383 - val_accuracy: 0.9860\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0633 - accuracy: 0.9824 - val_loss: 0.0391 - val_accuracy: 0.9910\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0391 - accuracy: 0.9910\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 107ms/step - loss: 0.5403 - accuracy: 0.6905 - val_loss: 0.0971 - val_accuracy: 0.9630\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1683 - accuracy: 0.9399 - val_loss: 0.1152 - val_accuracy: 0.9630\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1249 - accuracy: 0.9581 - val_loss: 0.1109 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1578 - accuracy: 0.9554 - val_loss: 0.0646 - val_accuracy: 0.9730\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1099 - accuracy: 0.9663 - val_loss: 0.1346 - val_accuracy: 0.9610\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1230 - accuracy: 0.9592 - val_loss: 0.0937 - val_accuracy: 0.9740\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1010 - accuracy: 0.9667 - val_loss: 0.0734 - val_accuracy: 0.9670\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0948 - accuracy: 0.9731 - val_loss: 0.0719 - val_accuracy: 0.9710\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0795 - accuracy: 0.9728 - val_loss: 0.0707 - val_accuracy: 0.9800\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0756 - accuracy: 0.9763 - val_loss: 0.0580 - val_accuracy: 0.9810\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0713 - accuracy: 0.9790 - val_loss: 0.0513 - val_accuracy: 0.9820\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.0608 - val_accuracy: 0.9830\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.0608 - accuracy: 0.9830\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 108ms/step - loss: 0.5780 - accuracy: 0.6988 - val_loss: 0.0674 - val_accuracy: 0.9760\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.1315 - accuracy: 0.9600 - val_loss: 0.0964 - val_accuracy: 0.9600\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1120 - accuracy: 0.9651 - val_loss: 0.0657 - val_accuracy: 0.9710\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1190 - accuracy: 0.9653 - val_loss: 0.1163 - val_accuracy: 0.9590\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1146 - accuracy: 0.9596 - val_loss: 0.0944 - val_accuracy: 0.9680\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 14s 112ms/step - loss: 0.0919 - accuracy: 0.9679 - val_loss: 0.0583 - val_accuracy: 0.9800\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0716 - accuracy: 0.9800 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0622 - accuracy: 0.9832 - val_loss: 0.2316 - val_accuracy: 0.9380\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1458 - accuracy: 0.9547 - val_loss: 0.0981 - val_accuracy: 0.9600\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0829 - accuracy: 0.9741 - val_loss: 0.0471 - val_accuracy: 0.9840\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.0432 - val_accuracy: 0.9840\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0594 - accuracy: 0.9824 - val_loss: 0.0464 - val_accuracy: 0.9870\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0464 - accuracy: 0.9870\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 16s 109ms/step - loss: 0.5256 - accuracy: 0.7181 - val_loss: 0.3415 - val_accuracy: 0.9040\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.3379 - accuracy: 0.9097 - val_loss: 0.2468 - val_accuracy: 0.9450\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.2293 - accuracy: 0.9396 - val_loss: 0.1572 - val_accuracy: 0.9590\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1275 - accuracy: 0.9618 - val_loss: 0.1074 - val_accuracy: 0.9590\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1249 - accuracy: 0.9587 - val_loss: 0.0862 - val_accuracy: 0.9710\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0948 - accuracy: 0.9676 - val_loss: 0.0684 - val_accuracy: 0.9730\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0905 - accuracy: 0.9706 - val_loss: 0.0942 - val_accuracy: 0.9690\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.1141 - accuracy: 0.9624 - val_loss: 0.0780 - val_accuracy: 0.9640\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0973 - accuracy: 0.9715 - val_loss: 0.0527 - val_accuracy: 0.9840\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0717 - accuracy: 0.9776 - val_loss: 0.0435 - val_accuracy: 0.9890\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.0741 - accuracy: 0.9795 - val_loss: 0.0430 - val_accuracy: 0.9880\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0602 - accuracy: 0.9818 - val_loss: 0.0387 - val_accuracy: 0.9900\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.0387 - accuracy: 0.9900\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 16s 112ms/step - loss: 0.5211 - accuracy: 0.7210 - val_loss: 0.1220 - val_accuracy: 0.9610\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1483 - accuracy: 0.9539 - val_loss: 0.1224 - val_accuracy: 0.9660\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.2503 - accuracy: 0.9250 - val_loss: 0.1178 - val_accuracy: 0.9570\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1231 - accuracy: 0.9587 - val_loss: 0.0768 - val_accuracy: 0.9690\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 0.0920 - accuracy: 0.9711 - val_loss: 0.0800 - val_accuracy: 0.9660\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.0773 - accuracy: 0.9748 - val_loss: 0.0556 - val_accuracy: 0.9780\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 14s 109ms/step - loss: 0.0809 - accuracy: 0.9735 - val_loss: 0.1066 - val_accuracy: 0.9580\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0968 - accuracy: 0.9688 - val_loss: 0.0570 - val_accuracy: 0.9810\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0678 - accuracy: 0.9811 - val_loss: 0.0526 - val_accuracy: 0.9830\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.0709 - accuracy: 0.9785 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0483 - accuracy: 0.9861 - val_loss: 0.0535 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0650 - accuracy: 0.9806 - val_loss: 0.0401 - val_accuracy: 0.9860\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.0401 - accuracy: 0.9860\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 161ms/step - loss: 0.9977 - accuracy: 0.5985 - val_loss: 0.7347 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 147ms/step - loss: 0.7407 - accuracy: 0.5691 - val_loss: 0.7025 - val_accuracy: 0.5600\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.6859 - accuracy: 0.5736 - val_loss: 0.6798 - val_accuracy: 0.5600\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.6807 - accuracy: 0.5872 - val_loss: 0.6780 - val_accuracy: 0.5600\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 0.6753 - accuracy: 0.5725 - val_loss: 0.6627 - val_accuracy: 0.5600\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 0.6603 - accuracy: 0.6160 - val_loss: 0.6426 - val_accuracy: 0.5600\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.6542 - accuracy: 0.6274 - val_loss: 0.6861 - val_accuracy: 0.5600\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.5242 - accuracy: 0.7694 - val_loss: 0.3183 - val_accuracy: 0.9450\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 167ms/step - loss: 0.2850 - accuracy: 0.9249 - val_loss: 0.2133 - val_accuracy: 0.9420\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 12s 187ms/step - loss: 0.1864 - accuracy: 0.9430 - val_loss: 0.1009 - val_accuracy: 0.9610\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 11s 167ms/step - loss: 0.1250 - accuracy: 0.9614 - val_loss: 1.2027 - val_accuracy: 0.4390\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.3796 - accuracy: 0.8606 - val_loss: 0.0857 - val_accuracy: 0.9740\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0857 - accuracy: 0.9740\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 166ms/step - loss: 0.5880 - accuracy: 0.6325 - val_loss: 0.2306 - val_accuracy: 0.9170\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.1290 - accuracy: 0.9595 - val_loss: 0.1127 - val_accuracy: 0.9610\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1248 - accuracy: 0.9628 - val_loss: 0.1455 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.1399 - accuracy: 0.9562 - val_loss: 0.0983 - val_accuracy: 0.9760\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1508 - accuracy: 0.9549 - val_loss: 0.0703 - val_accuracy: 0.9750\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0974 - accuracy: 0.9683 - val_loss: 0.0687 - val_accuracy: 0.9660\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0945 - accuracy: 0.9676 - val_loss: 0.0739 - val_accuracy: 0.9660\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 154ms/step - loss: 0.1079 - accuracy: 0.9623 - val_loss: 0.0552 - val_accuracy: 0.9770\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0908 - accuracy: 0.9687 - val_loss: 0.0982 - val_accuracy: 0.9690\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0666 - accuracy: 0.9772 - val_loss: 0.0744 - val_accuracy: 0.9750\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0903 - accuracy: 0.9739 - val_loss: 0.0667 - val_accuracy: 0.9770\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 10s 166ms/step - loss: 0.0663 - accuracy: 0.9781 - val_loss: 0.0683 - val_accuracy: 0.9790\n",
            "16/16 [==============================] - 1s 58ms/step - loss: 0.0683 - accuracy: 0.9790\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 13s 174ms/step - loss: 0.5786 - accuracy: 0.6673 - val_loss: 0.1427 - val_accuracy: 0.9630\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1157 - accuracy: 0.9630 - val_loss: 0.1108 - val_accuracy: 0.9630\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1131 - accuracy: 0.9605 - val_loss: 0.0721 - val_accuracy: 0.9770\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.1065 - accuracy: 0.9674 - val_loss: 0.0859 - val_accuracy: 0.9650\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0919 - accuracy: 0.9725 - val_loss: 0.0762 - val_accuracy: 0.9740\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0802 - accuracy: 0.9731 - val_loss: 0.0913 - val_accuracy: 0.9650\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1033 - accuracy: 0.9687 - val_loss: 0.0898 - val_accuracy: 0.9660\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.1039 - accuracy: 0.9676 - val_loss: 0.0654 - val_accuracy: 0.9800\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0781 - accuracy: 0.9792 - val_loss: 0.0498 - val_accuracy: 0.9840\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0856 - accuracy: 0.9733 - val_loss: 0.0548 - val_accuracy: 0.9800\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 9s 147ms/step - loss: 0.0744 - accuracy: 0.9766 - val_loss: 0.0691 - val_accuracy: 0.9800\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0815 - accuracy: 0.9786 - val_loss: 0.0503 - val_accuracy: 0.9840\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 0.0503 - accuracy: 0.9840\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 163ms/step - loss: 0.5924 - accuracy: 0.6576 - val_loss: 0.2611 - val_accuracy: 0.9500\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.2050 - accuracy: 0.9466 - val_loss: 0.1197 - val_accuracy: 0.9580\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1154 - accuracy: 0.9644 - val_loss: 0.0896 - val_accuracy: 0.9650\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 10s 166ms/step - loss: 0.1032 - accuracy: 0.9696 - val_loss: 0.1201 - val_accuracy: 0.9700\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.0972 - accuracy: 0.9708 - val_loss: 0.1583 - val_accuracy: 0.9550\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1138 - accuracy: 0.9578 - val_loss: 0.0649 - val_accuracy: 0.9790\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.0936 - accuracy: 0.9732 - val_loss: 0.1063 - val_accuracy: 0.9680\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.1025 - accuracy: 0.9647 - val_loss: 0.0650 - val_accuracy: 0.9840\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0648 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9780\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.0779 - accuracy: 0.9815 - val_loss: 0.0503 - val_accuracy: 0.9810\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.0824 - accuracy: 0.9756 - val_loss: 0.0666 - val_accuracy: 0.9800\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0909 - accuracy: 0.9740 - val_loss: 0.0473 - val_accuracy: 0.9860\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 0.0473 - accuracy: 0.9860\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 163ms/step - loss: 0.6158 - accuracy: 0.6368 - val_loss: 0.3678 - val_accuracy: 0.9520\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.2683 - accuracy: 0.9509 - val_loss: 0.1284 - val_accuracy: 0.9610\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1359 - accuracy: 0.9572 - val_loss: 0.0852 - val_accuracy: 0.9760\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1008 - accuracy: 0.9720 - val_loss: 0.0556 - val_accuracy: 0.9790\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0807 - accuracy: 0.9748 - val_loss: 0.0600 - val_accuracy: 0.9780\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0877 - accuracy: 0.9763 - val_loss: 0.0588 - val_accuracy: 0.9800\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0676 - accuracy: 0.9791 - val_loss: 0.0829 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0826 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 0.9830\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 164ms/step - loss: 0.0668 - accuracy: 0.9808 - val_loss: 0.0664 - val_accuracy: 0.9750\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 10s 160ms/step - loss: 0.0779 - accuracy: 0.9792 - val_loss: 0.0644 - val_accuracy: 0.9760\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0755 - accuracy: 0.9764 - val_loss: 0.1075 - val_accuracy: 0.9760\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0979 - accuracy: 0.9737 - val_loss: 0.0477 - val_accuracy: 0.9810\n",
            "16/16 [==============================] - 1s 56ms/step - loss: 0.0477 - accuracy: 0.9810\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 163ms/step - loss: 0.5836 - accuracy: 0.6503 - val_loss: 0.1354 - val_accuracy: 0.9660\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1201 - accuracy: 0.9630 - val_loss: 0.1194 - val_accuracy: 0.9680\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.1110 - val_accuracy: 0.9550\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1348 - accuracy: 0.9544 - val_loss: 0.0984 - val_accuracy: 0.9620\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1235 - accuracy: 0.9635 - val_loss: 0.1478 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 10s 154ms/step - loss: 0.1246 - accuracy: 0.9613 - val_loss: 0.0813 - val_accuracy: 0.9620\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.1168 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.0983 - accuracy: 0.9704 - val_loss: 0.0505 - val_accuracy: 0.9820\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0721 - accuracy: 0.9745 - val_loss: 0.0711 - val_accuracy: 0.9650\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0811 - accuracy: 0.9738 - val_loss: 0.0708 - val_accuracy: 0.9710\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 10s 161ms/step - loss: 0.0732 - accuracy: 0.9794 - val_loss: 0.0536 - val_accuracy: 0.9800\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.0715 - accuracy: 0.9778 - val_loss: 0.0462 - val_accuracy: 0.9840\n",
            "16/16 [==============================] - 1s 54ms/step - loss: 0.0462 - accuracy: 0.9840\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 162ms/step - loss: 0.5941 - accuracy: 0.6668 - val_loss: 0.2248 - val_accuracy: 0.9600\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 148ms/step - loss: 0.1724 - accuracy: 0.9561 - val_loss: 0.0827 - val_accuracy: 0.9720\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.1000 - accuracy: 0.9670 - val_loss: 0.0898 - val_accuracy: 0.9730\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.1218 - accuracy: 0.9591 - val_loss: 0.0912 - val_accuracy: 0.9640\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1070 - accuracy: 0.9636 - val_loss: 0.0716 - val_accuracy: 0.9700\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.0857 - accuracy: 0.9704 - val_loss: 0.0814 - val_accuracy: 0.9650\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0946 - accuracy: 0.9667 - val_loss: 0.0628 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.0658 - accuracy: 0.9775 - val_loss: 0.0900 - val_accuracy: 0.9710\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0895 - accuracy: 0.9702 - val_loss: 0.0598 - val_accuracy: 0.9820\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0856 - accuracy: 0.9744 - val_loss: 0.0584 - val_accuracy: 0.9820\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.0729 - accuracy: 0.9763 - val_loss: 0.0596 - val_accuracy: 0.9800\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0570 - accuracy: 0.9819 - val_loss: 0.0613 - val_accuracy: 0.9790\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0613 - accuracy: 0.9790\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 165ms/step - loss: 0.6893 - accuracy: 0.6227 - val_loss: 0.5352 - val_accuracy: 0.7180\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.4951 - accuracy: 0.8429 - val_loss: 0.4158 - val_accuracy: 0.8980\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.3805 - accuracy: 0.8996 - val_loss: 0.2905 - val_accuracy: 0.9490\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.2426 - accuracy: 0.9524 - val_loss: 0.0958 - val_accuracy: 0.9770\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 10s 161ms/step - loss: 0.1292 - accuracy: 0.9638 - val_loss: 0.1206 - val_accuracy: 0.9650\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 10s 161ms/step - loss: 0.1368 - accuracy: 0.9656 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.1009 - accuracy: 0.9727 - val_loss: 0.0750 - val_accuracy: 0.9790\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1160 - accuracy: 0.9679 - val_loss: 0.0684 - val_accuracy: 0.9810\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.1059 - accuracy: 0.9674 - val_loss: 0.0713 - val_accuracy: 0.9760\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1026 - accuracy: 0.9704 - val_loss: 0.0673 - val_accuracy: 0.9790\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0949 - accuracy: 0.9691 - val_loss: 0.0823 - val_accuracy: 0.9730\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.1213 - accuracy: 0.9639 - val_loss: 0.0721 - val_accuracy: 0.9760\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0721 - accuracy: 0.9760\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 12s 162ms/step - loss: 0.6031 - accuracy: 0.6565 - val_loss: 0.2102 - val_accuracy: 0.9590\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.1641 - accuracy: 0.9503 - val_loss: 0.1055 - val_accuracy: 0.9680\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.1493 - accuracy: 0.9510 - val_loss: 0.1278 - val_accuracy: 0.9630\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.1605 - accuracy: 0.9513 - val_loss: 0.0876 - val_accuracy: 0.9600\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.1233 - accuracy: 0.9611 - val_loss: 0.1032 - val_accuracy: 0.9680\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 10s 161ms/step - loss: 0.1152 - accuracy: 0.9627 - val_loss: 0.1568 - val_accuracy: 0.9580\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.1309 - accuracy: 0.9553 - val_loss: 0.1145 - val_accuracy: 0.9600\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 9s 151ms/step - loss: 0.1097 - accuracy: 0.9659 - val_loss: 0.0663 - val_accuracy: 0.9790\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0815 - accuracy: 0.9761 - val_loss: 0.0650 - val_accuracy: 0.9790\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0785 - accuracy: 0.9773 - val_loss: 0.0529 - val_accuracy: 0.9830\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0651 - accuracy: 0.9821 - val_loss: 0.0575 - val_accuracy: 0.9810\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.0768 - accuracy: 0.9773 - val_loss: 0.0640 - val_accuracy: 0.9770\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0640 - accuracy: 0.9770\n",
            "Epoch 1/12\n",
            "63/63 [==============================] - 13s 163ms/step - loss: 0.6212 - accuracy: 0.6492 - val_loss: 0.4667 - val_accuracy: 0.8510\n",
            "Epoch 2/12\n",
            "63/63 [==============================] - 9s 150ms/step - loss: 0.3780 - accuracy: 0.9002 - val_loss: 0.1271 - val_accuracy: 0.9610\n",
            "Epoch 3/12\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.1149 - accuracy: 0.9597 - val_loss: 0.0704 - val_accuracy: 0.9730\n",
            "Epoch 4/12\n",
            "63/63 [==============================] - 9s 149ms/step - loss: 0.0879 - accuracy: 0.9734 - val_loss: 0.0698 - val_accuracy: 0.9740\n",
            "Epoch 5/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.1085 - accuracy: 0.9620 - val_loss: 0.0821 - val_accuracy: 0.9610\n",
            "Epoch 6/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.0860 - accuracy: 0.9712 - val_loss: 0.1063 - val_accuracy: 0.9590\n",
            "Epoch 7/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.0931 - accuracy: 0.9673 - val_loss: 0.0868 - val_accuracy: 0.9650\n",
            "Epoch 8/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1050 - accuracy: 0.9650 - val_loss: 0.0703 - val_accuracy: 0.9750\n",
            "Epoch 9/12\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.0819 - accuracy: 0.9749 - val_loss: 0.0524 - val_accuracy: 0.9800\n",
            "Epoch 10/12\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0923 - accuracy: 0.9704 - val_loss: 0.0909 - val_accuracy: 0.9610\n",
            "Epoch 11/12\n",
            "63/63 [==============================] - 10s 153ms/step - loss: 0.1099 - accuracy: 0.9653 - val_loss: 0.1136 - val_accuracy: 0.9690\n",
            "Epoch 12/12\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.1122 - accuracy: 0.9676 - val_loss: 0.0583 - val_accuracy: 0.9800\n",
            "16/16 [==============================] - 1s 55ms/step - loss: 0.0583 - accuracy: 0.9800\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 13s 333ms/step - loss: 0.6404 - accuracy: 0.5903 - val_loss: 0.4230 - val_accuracy: 0.9060\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 320ms/step - loss: 0.3309 - accuracy: 0.9241 - val_loss: 0.1518 - val_accuracy: 0.9570\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 327ms/step - loss: 0.2391 - accuracy: 0.9426 - val_loss: 0.2241 - val_accuracy: 0.9540\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.2218 - accuracy: 0.9515 - val_loss: 0.1328 - val_accuracy: 0.9580\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1456 - accuracy: 0.9559 - val_loss: 0.1083 - val_accuracy: 0.9610\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 306ms/step - loss: 0.1054 - accuracy: 0.9698 - val_loss: 0.0927 - val_accuracy: 0.9640\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.1008 - accuracy: 0.9688 - val_loss: 0.0790 - val_accuracy: 0.9750\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0780 - accuracy: 0.9787 - val_loss: 0.0835 - val_accuracy: 0.9730\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0906 - accuracy: 0.9745 - val_loss: 0.0633 - val_accuracy: 0.9750\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0757 - accuracy: 0.9775 - val_loss: 0.0772 - val_accuracy: 0.9710\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0913 - accuracy: 0.9680 - val_loss: 0.0713 - val_accuracy: 0.9750\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.0774 - accuracy: 0.9747 - val_loss: 0.0606 - val_accuracy: 0.9760\n",
            "8/8 [==============================] - 1s 114ms/step - loss: 0.0606 - accuracy: 0.9760\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 14s 361ms/step - loss: 0.6450 - accuracy: 0.5860 - val_loss: 0.4388 - val_accuracy: 0.8810\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 9s 297ms/step - loss: 0.4218 - accuracy: 0.8953 - val_loss: 0.4035 - val_accuracy: 0.8180\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.2384 - accuracy: 0.9215 - val_loss: 0.1123 - val_accuracy: 0.9640\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.1023 - accuracy: 0.9667 - val_loss: 0.0909 - val_accuracy: 0.9640\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 9s 296ms/step - loss: 0.0869 - accuracy: 0.9685 - val_loss: 0.0894 - val_accuracy: 0.9690\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0845 - accuracy: 0.9705 - val_loss: 0.0654 - val_accuracy: 0.9730\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0755 - accuracy: 0.9740 - val_loss: 0.1019 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.1154 - accuracy: 0.9569 - val_loss: 0.0751 - val_accuracy: 0.9710\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0874 - accuracy: 0.9677 - val_loss: 0.0811 - val_accuracy: 0.9710\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0840 - accuracy: 0.9732 - val_loss: 0.0790 - val_accuracy: 0.9660\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0923 - accuracy: 0.9707 - val_loss: 0.0921 - val_accuracy: 0.9680\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0912 - accuracy: 0.9709 - val_loss: 0.0885 - val_accuracy: 0.9650\n",
            "8/8 [==============================] - 1s 111ms/step - loss: 0.0885 - accuracy: 0.9650\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 323ms/step - loss: 0.6441 - accuracy: 0.5977 - val_loss: 0.4104 - val_accuracy: 0.9160\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.3239 - accuracy: 0.9458 - val_loss: 0.2287 - val_accuracy: 0.9550\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.2097 - accuracy: 0.9570 - val_loss: 0.0875 - val_accuracy: 0.9660\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.1053 - accuracy: 0.9645 - val_loss: 0.0943 - val_accuracy: 0.9670\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1161 - accuracy: 0.9610 - val_loss: 0.1293 - val_accuracy: 0.9550\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1332 - accuracy: 0.9563 - val_loss: 0.0979 - val_accuracy: 0.9670\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1113 - accuracy: 0.9629 - val_loss: 0.0782 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.0925 - accuracy: 0.9682 - val_loss: 0.0765 - val_accuracy: 0.9730\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1035 - accuracy: 0.9669 - val_loss: 0.1031 - val_accuracy: 0.9720\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.0944 - accuracy: 0.9733 - val_loss: 0.0759 - val_accuracy: 0.9660\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.0783 - accuracy: 0.9730 - val_loss: 0.0591 - val_accuracy: 0.9730\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 322ms/step - loss: 0.0838 - accuracy: 0.9731 - val_loss: 0.0622 - val_accuracy: 0.9740\n",
            "8/8 [==============================] - 1s 115ms/step - loss: 0.0622 - accuracy: 0.9740\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 324ms/step - loss: 0.6568 - accuracy: 0.5651 - val_loss: 0.4897 - val_accuracy: 0.8220\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.5207 - accuracy: 0.8312 - val_loss: 0.4592 - val_accuracy: 0.8420\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 305ms/step - loss: 0.3906 - accuracy: 0.9138 - val_loss: 0.1423 - val_accuracy: 0.9620\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.1159 - accuracy: 0.9685 - val_loss: 0.1084 - val_accuracy: 0.9640\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 305ms/step - loss: 0.0943 - accuracy: 0.9701 - val_loss: 0.0774 - val_accuracy: 0.9720\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.1242 - accuracy: 0.9586 - val_loss: 0.0907 - val_accuracy: 0.9680\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0988 - accuracy: 0.9687 - val_loss: 0.0731 - val_accuracy: 0.9720\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 315ms/step - loss: 0.0853 - accuracy: 0.9717 - val_loss: 0.0907 - val_accuracy: 0.9600\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1195 - accuracy: 0.9629 - val_loss: 0.1223 - val_accuracy: 0.9580\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.0959 - accuracy: 0.9683 - val_loss: 0.0738 - val_accuracy: 0.9730\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 311ms/step - loss: 0.1116 - accuracy: 0.9659 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1116 - accuracy: 0.9663 - val_loss: 0.0820 - val_accuracy: 0.9730\n",
            "8/8 [==============================] - 1s 112ms/step - loss: 0.0820 - accuracy: 0.9730\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 325ms/step - loss: 0.6531 - accuracy: 0.5757 - val_loss: 0.4446 - val_accuracy: 0.9490\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.4113 - accuracy: 0.9065 - val_loss: 0.1180 - val_accuracy: 0.9640\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.1290 - accuracy: 0.9606 - val_loss: 0.1139 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1122 - accuracy: 0.9656 - val_loss: 0.0827 - val_accuracy: 0.9750\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1109 - accuracy: 0.9626 - val_loss: 0.1286 - val_accuracy: 0.9640\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1058 - accuracy: 0.9660 - val_loss: 0.1120 - val_accuracy: 0.9620\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.1043 - accuracy: 0.9681 - val_loss: 0.0710 - val_accuracy: 0.9740\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0861 - accuracy: 0.9734 - val_loss: 0.0806 - val_accuracy: 0.9700\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0822 - accuracy: 0.9727 - val_loss: 0.0861 - val_accuracy: 0.9720\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0693 - accuracy: 0.9775 - val_loss: 0.0560 - val_accuracy: 0.9800\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 310ms/step - loss: 0.0971 - accuracy: 0.9688 - val_loss: 0.0699 - val_accuracy: 0.9750\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 297ms/step - loss: 0.0849 - accuracy: 0.9716 - val_loss: 0.0698 - val_accuracy: 0.9750\n",
            "8/8 [==============================] - 1s 115ms/step - loss: 0.0698 - accuracy: 0.9750\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 322ms/step - loss: 0.6420 - accuracy: 0.5841 - val_loss: 0.4376 - val_accuracy: 0.9010\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 304ms/step - loss: 0.3309 - accuracy: 0.9145 - val_loss: 0.1465 - val_accuracy: 0.9620\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1358 - accuracy: 0.9624 - val_loss: 0.0989 - val_accuracy: 0.9590\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.1125 - accuracy: 0.9626 - val_loss: 0.1044 - val_accuracy: 0.9720\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 303ms/step - loss: 0.1152 - accuracy: 0.9625 - val_loss: 0.1015 - val_accuracy: 0.9650\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 306ms/step - loss: 0.1193 - accuracy: 0.9600 - val_loss: 0.1067 - val_accuracy: 0.9590\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.0890 - accuracy: 0.9719 - val_loss: 0.0744 - val_accuracy: 0.9750\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 314ms/step - loss: 0.0893 - accuracy: 0.9726 - val_loss: 0.0783 - val_accuracy: 0.9740\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.0933 - accuracy: 0.9680 - val_loss: 0.0713 - val_accuracy: 0.9690\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1221 - accuracy: 0.9606 - val_loss: 0.0644 - val_accuracy: 0.9770\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.0799 - accuracy: 0.9733 - val_loss: 0.0687 - val_accuracy: 0.9740\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 9s 295ms/step - loss: 0.1135 - accuracy: 0.9610 - val_loss: 0.0620 - val_accuracy: 0.9750\n",
            "8/8 [==============================] - 1s 114ms/step - loss: 0.0620 - accuracy: 0.9750\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 323ms/step - loss: 0.6510 - accuracy: 0.5928 - val_loss: 0.4493 - val_accuracy: 0.8810\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.4491 - accuracy: 0.8775 - val_loss: 0.1389 - val_accuracy: 0.9670\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 316ms/step - loss: 0.1637 - accuracy: 0.9502 - val_loss: 0.0689 - val_accuracy: 0.9710\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0909 - accuracy: 0.9688 - val_loss: 0.1107 - val_accuracy: 0.9650\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1125 - accuracy: 0.9665 - val_loss: 0.1044 - val_accuracy: 0.9730\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 303ms/step - loss: 0.1426 - accuracy: 0.9575 - val_loss: 0.1362 - val_accuracy: 0.9670\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.1197 - accuracy: 0.9625 - val_loss: 0.0632 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 9s 297ms/step - loss: 0.0992 - accuracy: 0.9664 - val_loss: 0.0792 - val_accuracy: 0.9750\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.0768 - accuracy: 0.9728 - val_loss: 0.1192 - val_accuracy: 0.9640\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 303ms/step - loss: 0.0923 - accuracy: 0.9728 - val_loss: 0.0680 - val_accuracy: 0.9760\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 308ms/step - loss: 0.0896 - accuracy: 0.9700 - val_loss: 0.0678 - val_accuracy: 0.9750\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0753 - accuracy: 0.9770 - val_loss: 0.1000 - val_accuracy: 0.9690\n",
            "8/8 [==============================] - 1s 114ms/step - loss: 0.1000 - accuracy: 0.9690\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 322ms/step - loss: 0.6385 - accuracy: 0.6026 - val_loss: 0.4001 - val_accuracy: 0.8890\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 9s 296ms/step - loss: 0.3786 - accuracy: 0.9064 - val_loss: 0.2951 - val_accuracy: 0.9510\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.2542 - accuracy: 0.9518 - val_loss: 0.0796 - val_accuracy: 0.9710\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0872 - accuracy: 0.9741 - val_loss: 0.0718 - val_accuracy: 0.9750\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1070 - accuracy: 0.9639 - val_loss: 0.0823 - val_accuracy: 0.9730\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0984 - accuracy: 0.9702 - val_loss: 0.1210 - val_accuracy: 0.9670\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0918 - accuracy: 0.9710 - val_loss: 0.0836 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0750 - accuracy: 0.9751 - val_loss: 0.0932 - val_accuracy: 0.9620\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 307ms/step - loss: 0.1035 - accuracy: 0.9656 - val_loss: 0.0758 - val_accuracy: 0.9740\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1099 - accuracy: 0.9654 - val_loss: 0.0678 - val_accuracy: 0.9750\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 9s 297ms/step - loss: 0.0749 - accuracy: 0.9761 - val_loss: 0.0672 - val_accuracy: 0.9740\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 9s 295ms/step - loss: 0.0963 - accuracy: 0.9718 - val_loss: 0.0929 - val_accuracy: 0.9660\n",
            "8/8 [==============================] - 1s 109ms/step - loss: 0.0929 - accuracy: 0.9660\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 324ms/step - loss: 0.6508 - accuracy: 0.5600 - val_loss: 0.8673 - val_accuracy: 0.4800\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.5728 - accuracy: 0.7894 - val_loss: 0.3913 - val_accuracy: 0.9170\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 313ms/step - loss: 0.3398 - accuracy: 0.9362 - val_loss: 0.1368 - val_accuracy: 0.9560\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 312ms/step - loss: 0.1308 - accuracy: 0.9556 - val_loss: 0.0782 - val_accuracy: 0.9690\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 306ms/step - loss: 0.0949 - accuracy: 0.9715 - val_loss: 0.1014 - val_accuracy: 0.9680\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1313 - accuracy: 0.9582 - val_loss: 0.0678 - val_accuracy: 0.9730\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 302ms/step - loss: 0.0842 - accuracy: 0.9720 - val_loss: 0.1022 - val_accuracy: 0.9640\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.2189 - accuracy: 0.9287 - val_loss: 0.1359 - val_accuracy: 0.9610\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1267 - accuracy: 0.9578 - val_loss: 0.0769 - val_accuracy: 0.9730\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 323ms/step - loss: 0.0932 - accuracy: 0.9664 - val_loss: 0.0928 - val_accuracy: 0.9670\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.0873 - accuracy: 0.9705 - val_loss: 0.0818 - val_accuracy: 0.9700\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 297ms/step - loss: 0.0906 - accuracy: 0.9697 - val_loss: 0.1080 - val_accuracy: 0.9600\n",
            "8/8 [==============================] - 1s 114ms/step - loss: 0.1080 - accuracy: 0.9600\n",
            "Epoch 1/12\n",
            "32/32 [==============================] - 12s 323ms/step - loss: 0.6604 - accuracy: 0.5450 - val_loss: 0.6910 - val_accuracy: 0.4320\n",
            "Epoch 2/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.5062 - accuracy: 0.8097 - val_loss: 0.3212 - val_accuracy: 0.9490\n",
            "Epoch 3/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.2759 - accuracy: 0.9405 - val_loss: 0.3686 - val_accuracy: 0.8870\n",
            "Epoch 4/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.3734 - accuracy: 0.8939 - val_loss: 0.3030 - val_accuracy: 0.9240\n",
            "Epoch 5/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.2923 - accuracy: 0.9162 - val_loss: 0.2361 - val_accuracy: 0.9410\n",
            "Epoch 6/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.2332 - accuracy: 0.9324 - val_loss: 0.2120 - val_accuracy: 0.9480\n",
            "Epoch 7/12\n",
            "32/32 [==============================] - 10s 301ms/step - loss: 0.2172 - accuracy: 0.9379 - val_loss: 0.1850 - val_accuracy: 0.9530\n",
            "Epoch 8/12\n",
            "32/32 [==============================] - 10s 299ms/step - loss: 0.1795 - accuracy: 0.9466 - val_loss: 0.1432 - val_accuracy: 0.9590\n",
            "Epoch 9/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.1434 - accuracy: 0.9549 - val_loss: 0.0876 - val_accuracy: 0.9640\n",
            "Epoch 10/12\n",
            "32/32 [==============================] - 10s 309ms/step - loss: 0.1122 - accuracy: 0.9657 - val_loss: 0.0805 - val_accuracy: 0.9700\n",
            "Epoch 11/12\n",
            "32/32 [==============================] - 10s 300ms/step - loss: 0.0906 - accuracy: 0.9694 - val_loss: 0.0790 - val_accuracy: 0.9660\n",
            "Epoch 12/12\n",
            "32/32 [==============================] - 10s 298ms/step - loss: 0.0933 - accuracy: 0.9656 - val_loss: 0.0598 - val_accuracy: 0.9720\n",
            "8/8 [==============================] - 1s 113ms/step - loss: 0.0598 - accuracy: 0.9720\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 562ms/step - loss: 0.6725 - accuracy: 0.5599 - val_loss: 0.6327 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.5677 - accuracy: 0.6428 - val_loss: 0.3870 - val_accuracy: 0.9500\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 510ms/step - loss: 0.3512 - accuracy: 0.9409 - val_loss: 0.3032 - val_accuracy: 0.9390\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 516ms/step - loss: 0.3121 - accuracy: 0.9338 - val_loss: 0.2293 - val_accuracy: 0.9540\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 508ms/step - loss: 0.2053 - accuracy: 0.9576 - val_loss: 0.1167 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 515ms/step - loss: 0.1078 - accuracy: 0.9663 - val_loss: 0.1289 - val_accuracy: 0.9670\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 505ms/step - loss: 0.1041 - accuracy: 0.9680 - val_loss: 0.0659 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 512ms/step - loss: 0.0890 - accuracy: 0.9721 - val_loss: 0.0732 - val_accuracy: 0.9730\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 520ms/step - loss: 0.0847 - accuracy: 0.9729 - val_loss: 0.0827 - val_accuracy: 0.9760\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.0822 - accuracy: 0.9745 - val_loss: 0.0704 - val_accuracy: 0.9760\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 508ms/step - loss: 0.0826 - accuracy: 0.9738 - val_loss: 0.0658 - val_accuracy: 0.9760\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.0694 - accuracy: 0.9804 - val_loss: 0.0625 - val_accuracy: 0.9750\n",
            "4/4 [==============================] - 1s 181ms/step - loss: 0.0625 - accuracy: 0.9750\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 592ms/step - loss: 0.6750 - accuracy: 0.5251 - val_loss: 0.6250 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 9s 535ms/step - loss: 0.5738 - accuracy: 0.6494 - val_loss: 0.4240 - val_accuracy: 0.9230\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 9s 538ms/step - loss: 0.4465 - accuracy: 0.9051 - val_loss: 0.3388 - val_accuracy: 0.9330\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 509ms/step - loss: 0.2788 - accuracy: 0.9494 - val_loss: 0.1037 - val_accuracy: 0.9660\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 507ms/step - loss: 0.1105 - accuracy: 0.9631 - val_loss: 0.0968 - val_accuracy: 0.9700\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 517ms/step - loss: 0.0921 - accuracy: 0.9723 - val_loss: 0.0800 - val_accuracy: 0.9770\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 520ms/step - loss: 0.0931 - accuracy: 0.9712 - val_loss: 0.0831 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 9s 554ms/step - loss: 0.0810 - accuracy: 0.9754 - val_loss: 0.0692 - val_accuracy: 0.9730\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.0902 - accuracy: 0.9698 - val_loss: 0.0848 - val_accuracy: 0.9690\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 508ms/step - loss: 0.0867 - accuracy: 0.9731 - val_loss: 0.0702 - val_accuracy: 0.9730\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.0767 - accuracy: 0.9726 - val_loss: 0.0764 - val_accuracy: 0.9710\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0851 - accuracy: 0.9723 - val_loss: 0.0701 - val_accuracy: 0.9760\n",
            "4/4 [==============================] - 1s 181ms/step - loss: 0.0701 - accuracy: 0.9760\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 562ms/step - loss: 0.6594 - accuracy: 0.5759 - val_loss: 0.6081 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 512ms/step - loss: 0.5537 - accuracy: 0.6899 - val_loss: 0.4792 - val_accuracy: 0.9230\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 508ms/step - loss: 0.5245 - accuracy: 0.8647 - val_loss: 0.4870 - val_accuracy: 0.8490\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.4778 - accuracy: 0.8198 - val_loss: 0.4171 - val_accuracy: 0.8870\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 523ms/step - loss: 0.3987 - accuracy: 0.8982 - val_loss: 0.3117 - val_accuracy: 0.9420\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.3038 - accuracy: 0.9354 - val_loss: 0.1974 - val_accuracy: 0.9590\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.1724 - accuracy: 0.9568 - val_loss: 0.0936 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.0805 - accuracy: 0.9751 - val_loss: 0.1263 - val_accuracy: 0.9670\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.1124 - accuracy: 0.9660 - val_loss: 0.1031 - val_accuracy: 0.9680\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.1170 - accuracy: 0.9584 - val_loss: 0.0780 - val_accuracy: 0.9740\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0915 - accuracy: 0.9680 - val_loss: 0.0635 - val_accuracy: 0.9740\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 507ms/step - loss: 0.0769 - accuracy: 0.9765 - val_loss: 0.0775 - val_accuracy: 0.9760\n",
            "4/4 [==============================] - 1s 183ms/step - loss: 0.0775 - accuracy: 0.9760\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 559ms/step - loss: 0.6595 - accuracy: 0.6013 - val_loss: 0.6105 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.5410 - accuracy: 0.6940 - val_loss: 0.1893 - val_accuracy: 0.9440\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.5232 - accuracy: 0.8194 - val_loss: 0.5154 - val_accuracy: 0.8490\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 522ms/step - loss: 0.5058 - accuracy: 0.8513 - val_loss: 0.4739 - val_accuracy: 0.7920\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.4602 - accuracy: 0.8257 - val_loss: 0.4027 - val_accuracy: 0.9010\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.4025 - accuracy: 0.9041 - val_loss: 0.3381 - val_accuracy: 0.9320\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.3186 - accuracy: 0.9393 - val_loss: 0.2680 - val_accuracy: 0.9470\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.2643 - accuracy: 0.9405 - val_loss: 0.2046 - val_accuracy: 0.9560\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.2137 - accuracy: 0.9434 - val_loss: 0.1337 - val_accuracy: 0.9610\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.1238 - accuracy: 0.9613 - val_loss: 0.0773 - val_accuracy: 0.9660\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.0901 - accuracy: 0.9706 - val_loss: 0.0735 - val_accuracy: 0.9700\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.0893 - accuracy: 0.9678 - val_loss: 0.0724 - val_accuracy: 0.9730\n",
            "4/4 [==============================] - 1s 182ms/step - loss: 0.0724 - accuracy: 0.9730\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 563ms/step - loss: 0.6701 - accuracy: 0.5536 - val_loss: 0.6174 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 521ms/step - loss: 0.5583 - accuracy: 0.6586 - val_loss: 0.4296 - val_accuracy: 0.9230\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 512ms/step - loss: 0.4229 - accuracy: 0.9238 - val_loss: 0.2816 - val_accuracy: 0.9530\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 524ms/step - loss: 0.2403 - accuracy: 0.9531 - val_loss: 0.1057 - val_accuracy: 0.9680\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 524ms/step - loss: 0.1773 - accuracy: 0.9466 - val_loss: 0.1526 - val_accuracy: 0.9620\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 507ms/step - loss: 0.1324 - accuracy: 0.9597 - val_loss: 0.0773 - val_accuracy: 0.9720\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.0854 - accuracy: 0.9731 - val_loss: 0.1412 - val_accuracy: 0.9620\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 527ms/step - loss: 0.1306 - accuracy: 0.9610 - val_loss: 0.1311 - val_accuracy: 0.9660\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 505ms/step - loss: 0.1063 - accuracy: 0.9659 - val_loss: 0.0900 - val_accuracy: 0.9760\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 505ms/step - loss: 0.0620 - accuracy: 0.9811 - val_loss: 0.0708 - val_accuracy: 0.9670\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.0939 - accuracy: 0.9700 - val_loss: 0.1260 - val_accuracy: 0.9640\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.1194 - accuracy: 0.9637 - val_loss: 0.0785 - val_accuracy: 0.9630\n",
            "4/4 [==============================] - 1s 182ms/step - loss: 0.0785 - accuracy: 0.9630\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 562ms/step - loss: 0.6638 - accuracy: 0.5660 - val_loss: 0.6205 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 523ms/step - loss: 0.5979 - accuracy: 0.6346 - val_loss: 0.4123 - val_accuracy: 0.9300\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.4541 - accuracy: 0.9375 - val_loss: 0.4646 - val_accuracy: 0.9330\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.4563 - accuracy: 0.9143 - val_loss: 0.3886 - val_accuracy: 0.9200\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.3685 - accuracy: 0.9360 - val_loss: 0.2406 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.2300 - accuracy: 0.9537 - val_loss: 0.1257 - val_accuracy: 0.9630\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.1653 - accuracy: 0.9535 - val_loss: 0.1387 - val_accuracy: 0.9600\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.1363 - accuracy: 0.9618 - val_loss: 0.1312 - val_accuracy: 0.9580\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.1514 - accuracy: 0.9546 - val_loss: 0.1395 - val_accuracy: 0.9630\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.1410 - accuracy: 0.9601 - val_loss: 0.0840 - val_accuracy: 0.9770\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 497ms/step - loss: 0.1092 - accuracy: 0.9700 - val_loss: 0.0679 - val_accuracy: 0.9820\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.0998 - accuracy: 0.9738 - val_loss: 0.0699 - val_accuracy: 0.9820\n",
            "4/4 [==============================] - 1s 185ms/step - loss: 0.0699 - accuracy: 0.9820\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 580ms/step - loss: 0.6776 - accuracy: 0.5329 - val_loss: 0.6412 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 506ms/step - loss: 0.6012 - accuracy: 0.6102 - val_loss: 0.4166 - val_accuracy: 0.9440\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.4295 - accuracy: 0.9388 - val_loss: 0.2926 - val_accuracy: 0.9480\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.2470 - accuracy: 0.9534 - val_loss: 0.0863 - val_accuracy: 0.9730\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.1157 - accuracy: 0.9655 - val_loss: 0.0684 - val_accuracy: 0.9720\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 495ms/step - loss: 0.0827 - accuracy: 0.9727 - val_loss: 0.0995 - val_accuracy: 0.9680\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.0918 - accuracy: 0.9713 - val_loss: 0.0620 - val_accuracy: 0.9760\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.0807 - accuracy: 0.9752 - val_loss: 0.0799 - val_accuracy: 0.9740\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 500ms/step - loss: 0.0911 - accuracy: 0.9679 - val_loss: 0.0637 - val_accuracy: 0.9720\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.0888 - accuracy: 0.9703 - val_loss: 0.0863 - val_accuracy: 0.9620\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.0919 - accuracy: 0.9686 - val_loss: 0.1077 - val_accuracy: 0.9640\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.1027 - accuracy: 0.9663 - val_loss: 0.0596 - val_accuracy: 0.9760\n",
            "4/4 [==============================] - 1s 189ms/step - loss: 0.0596 - accuracy: 0.9760\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 557ms/step - loss: 0.6720 - accuracy: 0.5356 - val_loss: 0.6153 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 521ms/step - loss: 0.5436 - accuracy: 0.6656 - val_loss: 0.3638 - val_accuracy: 0.9130\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 503ms/step - loss: 0.3378 - accuracy: 0.9091 - val_loss: 0.2803 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 496ms/step - loss: 0.2588 - accuracy: 0.9577 - val_loss: 0.1751 - val_accuracy: 0.9560\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 509ms/step - loss: 0.1720 - accuracy: 0.9519 - val_loss: 0.1268 - val_accuracy: 0.9570\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 518ms/step - loss: 0.1099 - accuracy: 0.9634 - val_loss: 0.0735 - val_accuracy: 0.9700\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 521ms/step - loss: 0.1051 - accuracy: 0.9672 - val_loss: 0.0797 - val_accuracy: 0.9730\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 521ms/step - loss: 0.0807 - accuracy: 0.9743 - val_loss: 0.0581 - val_accuracy: 0.9770\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 9s 547ms/step - loss: 0.0838 - accuracy: 0.9763 - val_loss: 0.0689 - val_accuracy: 0.9750\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0929 - accuracy: 0.9720 - val_loss: 0.0734 - val_accuracy: 0.9760\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.0759 - accuracy: 0.9760 - val_loss: 0.0577 - val_accuracy: 0.9780\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0725 - accuracy: 0.9746 - val_loss: 0.0735 - val_accuracy: 0.9690\n",
            "4/4 [==============================] - 1s 191ms/step - loss: 0.0735 - accuracy: 0.9690\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 565ms/step - loss: 0.6573 - accuracy: 0.5911 - val_loss: 0.5992 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 516ms/step - loss: 0.5315 - accuracy: 0.6876 - val_loss: 0.4319 - val_accuracy: 0.9200\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.4265 - accuracy: 0.9063 - val_loss: 0.3335 - val_accuracy: 0.9350\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.2960 - accuracy: 0.9402 - val_loss: 0.1632 - val_accuracy: 0.9550\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 504ms/step - loss: 0.1243 - accuracy: 0.9619 - val_loss: 0.0920 - val_accuracy: 0.9690\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0985 - accuracy: 0.9702 - val_loss: 0.0798 - val_accuracy: 0.9710\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0917 - accuracy: 0.9705 - val_loss: 0.0657 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0853 - accuracy: 0.9751 - val_loss: 0.0952 - val_accuracy: 0.9660\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.1071 - accuracy: 0.9621 - val_loss: 0.0768 - val_accuracy: 0.9730\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0940 - accuracy: 0.9712 - val_loss: 0.0901 - val_accuracy: 0.9640\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.1052 - accuracy: 0.9597 - val_loss: 0.0764 - val_accuracy: 0.9710\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0883 - accuracy: 0.9710 - val_loss: 0.0724 - val_accuracy: 0.9730\n",
            "4/4 [==============================] - 1s 185ms/step - loss: 0.0724 - accuracy: 0.9730\n",
            "Epoch 1/12\n",
            "16/16 [==============================] - 11s 553ms/step - loss: 0.6569 - accuracy: 0.6025 - val_loss: 0.6074 - val_accuracy: 0.5600\n",
            "Epoch 2/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.5359 - accuracy: 0.6611 - val_loss: 0.3723 - val_accuracy: 0.9290\n",
            "Epoch 3/12\n",
            "16/16 [==============================] - 8s 523ms/step - loss: 0.3242 - accuracy: 0.9356 - val_loss: 0.1020 - val_accuracy: 0.9780\n",
            "Epoch 4/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.1318 - accuracy: 0.9647 - val_loss: 0.1087 - val_accuracy: 0.9740\n",
            "Epoch 5/12\n",
            "16/16 [==============================] - 8s 501ms/step - loss: 0.1047 - accuracy: 0.9700 - val_loss: 0.0766 - val_accuracy: 0.9720\n",
            "Epoch 6/12\n",
            "16/16 [==============================] - 8s 494ms/step - loss: 0.0892 - accuracy: 0.9707 - val_loss: 0.0660 - val_accuracy: 0.9730\n",
            "Epoch 7/12\n",
            "16/16 [==============================] - 8s 495ms/step - loss: 0.0809 - accuracy: 0.9732 - val_loss: 0.0663 - val_accuracy: 0.9710\n",
            "Epoch 8/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0887 - accuracy: 0.9709 - val_loss: 0.0883 - val_accuracy: 0.9740\n",
            "Epoch 9/12\n",
            "16/16 [==============================] - 8s 499ms/step - loss: 0.0978 - accuracy: 0.9707 - val_loss: 0.0719 - val_accuracy: 0.9670\n",
            "Epoch 10/12\n",
            "16/16 [==============================] - 8s 496ms/step - loss: 0.0876 - accuracy: 0.9689 - val_loss: 0.0590 - val_accuracy: 0.9790\n",
            "Epoch 11/12\n",
            "16/16 [==============================] - 8s 498ms/step - loss: 0.0653 - accuracy: 0.9771 - val_loss: 0.0637 - val_accuracy: 0.9760\n",
            "Epoch 12/12\n",
            "16/16 [==============================] - 8s 502ms/step - loss: 0.0755 - accuracy: 0.9747 - val_loss: 0.0976 - val_accuracy: 0.9680\n",
            "4/4 [==============================] - 1s 183ms/step - loss: 0.0976 - accuracy: 0.9680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fb384fa3ba8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4db710>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a519518>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb384dd7128>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb385b85588>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4c57f0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fb3858a63c8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb385b29668>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4db0f0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a22f2b0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb387a1e390>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb387a1e5f8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382e57ac8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382e570b8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb385b85c50>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4c5b00>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382ee9ba8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb3867f84a8>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fb38aebf278>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a5192e8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb384dd7630>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382e573c8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4c5cf8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382f33c88>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fb386553ac8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a519128>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb387a1ec88>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382e57c50>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4c5dd8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb382f33e10>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fb384fa3cc0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb384ea1c88>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4db320>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4db828>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a519438>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb387a1ecc0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb384dd7320>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb384dd71d0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb385b859b0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb385b850f0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb38a4c55f8>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb39218d668>]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pWHtS2kNus"
      },
      "source": [
        "32 seems like the best choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV4_xl3GkXpD"
      },
      "source": [
        "Much of the above code was taken from Jason Brownlee (https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/) and manipulated for the ECG data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b10SSMQGkd7W"
      },
      "source": [
        "# Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-Sv0Yi8kirb"
      },
      "source": [
        "Performance Metrics for data provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVY86C9Gkm4j",
        "outputId": "ee964569-fe35-4cd0-fa23-e4e2b244a234"
      },
      "source": [
        "#initiate performance metrics lists.\r\n",
        "accuracy1 = []*10\r\n",
        "precision1 = []*10\r\n",
        "recall1 = []*10\r\n",
        "f1_1 = []*10\r\n",
        "\r\n",
        "#repeat 10 times for accuracy.\r\n",
        "for i in range(10):\r\n",
        "  #build model and predict for the test data.\r\n",
        "  model1 = buildmodel(12, 32)\r\n",
        "  #calculate and record performance metrics.\r\n",
        "  pred1 = model1.predict_classes(test_data)\r\n",
        "  accuracy1.append(accuracy_score(test_labels, pred1))\r\n",
        "  precision1.append(precision_score(test_labels, pred1))\r\n",
        "  recall1.append(recall_score(test_labels, pred1))\r\n",
        "  f1_1.append(f1_score(test_labels, pred1))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 102ms/step - loss: 0.5450 - accuracy: 0.7126 - val_loss: 0.1418 - val_accuracy: 0.9600\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1384 - accuracy: 0.9570 - val_loss: 0.0936 - val_accuracy: 0.9620\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1211 - accuracy: 0.9618 - val_loss: 0.0605 - val_accuracy: 0.9770\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1033 - accuracy: 0.9610 - val_loss: 0.0664 - val_accuracy: 0.9820\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0794 - accuracy: 0.9754 - val_loss: 0.0505 - val_accuracy: 0.9840\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0666 - accuracy: 0.9798 - val_loss: 0.0745 - val_accuracy: 0.9720\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0821 - accuracy: 0.9761 - val_loss: 0.0705 - val_accuracy: 0.9770\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0807 - accuracy: 0.9782 - val_loss: 0.0455 - val_accuracy: 0.9850\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0743 - accuracy: 0.9757 - val_loss: 0.0457 - val_accuracy: 0.9850\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 96ms/step - loss: 0.0860 - accuracy: 0.9739 - val_loss: 0.0412 - val_accuracy: 0.9880\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0635 - accuracy: 0.9820 - val_loss: 0.0370 - val_accuracy: 0.9890\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0621 - accuracy: 0.9813 - val_loss: 0.0412 - val_accuracy: 0.9860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 102ms/step - loss: 0.4740 - accuracy: 0.7534 - val_loss: 0.1300 - val_accuracy: 0.9590\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1111 - accuracy: 0.9625 - val_loss: 0.0921 - val_accuracy: 0.9640\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1141 - accuracy: 0.9599 - val_loss: 0.2036 - val_accuracy: 0.9560\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 105ms/step - loss: 0.1815 - accuracy: 0.9476 - val_loss: 0.1127 - val_accuracy: 0.9620\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1078 - accuracy: 0.9664 - val_loss: 0.0881 - val_accuracy: 0.9650\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1577 - accuracy: 0.9496 - val_loss: 0.0875 - val_accuracy: 0.9730\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1006 - accuracy: 0.9689 - val_loss: 0.0828 - val_accuracy: 0.9680\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 96ms/step - loss: 0.0853 - accuracy: 0.9737 - val_loss: 0.0595 - val_accuracy: 0.9810\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0734 - accuracy: 0.9769 - val_loss: 0.0456 - val_accuracy: 0.9880\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1032 - accuracy: 0.9709 - val_loss: 0.1077 - val_accuracy: 0.9590\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 96ms/step - loss: 0.1348 - accuracy: 0.9587 - val_loss: 0.0548 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 96ms/step - loss: 0.0801 - accuracy: 0.9767 - val_loss: 0.0433 - val_accuracy: 0.9860\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 14s 102ms/step - loss: 0.5264 - accuracy: 0.7022 - val_loss: 0.1809 - val_accuracy: 0.9590\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.2163 - accuracy: 0.9345 - val_loss: 0.2634 - val_accuracy: 0.9180\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.2509 - accuracy: 0.9240 - val_loss: 0.1996 - val_accuracy: 0.9470\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.2117 - accuracy: 0.9384 - val_loss: 0.1676 - val_accuracy: 0.9530\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1609 - accuracy: 0.9490 - val_loss: 0.1100 - val_accuracy: 0.9600\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1061 - accuracy: 0.9645 - val_loss: 0.0660 - val_accuracy: 0.9680\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0917 - accuracy: 0.9639 - val_loss: 0.0548 - val_accuracy: 0.9800\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0824 - accuracy: 0.9759 - val_loss: 0.0508 - val_accuracy: 0.9830\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0751 - accuracy: 0.9774 - val_loss: 0.0422 - val_accuracy: 0.9870\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0709 - accuracy: 0.9779 - val_loss: 0.0470 - val_accuracy: 0.9860\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0567 - accuracy: 0.9835 - val_loss: 0.0354 - val_accuracy: 0.9890\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0491 - accuracy: 0.9838 - val_loss: 0.0458 - val_accuracy: 0.9840\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 104ms/step - loss: 0.5271 - accuracy: 0.7006 - val_loss: 0.0945 - val_accuracy: 0.9650\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.2739 - accuracy: 0.9300 - val_loss: 0.2139 - val_accuracy: 0.9490\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.2137 - accuracy: 0.9381 - val_loss: 0.1131 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.1897 - accuracy: 0.9407 - val_loss: 0.2032 - val_accuracy: 0.9440\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.2158 - accuracy: 0.9368 - val_loss: 0.1198 - val_accuracy: 0.9590\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.1133 - accuracy: 0.9615 - val_loss: 0.0714 - val_accuracy: 0.9650\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0944 - accuracy: 0.9658 - val_loss: 0.0657 - val_accuracy: 0.9760\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0823 - accuracy: 0.9750 - val_loss: 0.0461 - val_accuracy: 0.9860\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0723 - accuracy: 0.9783 - val_loss: 0.0651 - val_accuracy: 0.9820\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0775 - accuracy: 0.9797 - val_loss: 0.0373 - val_accuracy: 0.9900\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0760 - accuracy: 0.9773 - val_loss: 0.0418 - val_accuracy: 0.9880\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.0580 - accuracy: 0.9813 - val_loss: 0.0738 - val_accuracy: 0.9730\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 105ms/step - loss: 0.5348 - accuracy: 0.7064 - val_loss: 0.1986 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1414 - accuracy: 0.9615 - val_loss: 0.0912 - val_accuracy: 0.9740\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1099 - accuracy: 0.9618 - val_loss: 0.1608 - val_accuracy: 0.9590\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1195 - accuracy: 0.9596 - val_loss: 0.1288 - val_accuracy: 0.9610\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1462 - accuracy: 0.9535 - val_loss: 0.1283 - val_accuracy: 0.9590\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1128 - accuracy: 0.9667 - val_loss: 0.0995 - val_accuracy: 0.9600\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0951 - accuracy: 0.9711 - val_loss: 0.0524 - val_accuracy: 0.9820\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0712 - accuracy: 0.9763 - val_loss: 0.0660 - val_accuracy: 0.9790\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0890 - accuracy: 0.9731 - val_loss: 0.0428 - val_accuracy: 0.9860\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0661 - accuracy: 0.9796 - val_loss: 0.0405 - val_accuracy: 0.9890\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0677 - accuracy: 0.9775 - val_loss: 0.0643 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0616 - accuracy: 0.9830 - val_loss: 0.0490 - val_accuracy: 0.9860\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 107ms/step - loss: 0.5305 - accuracy: 0.7005 - val_loss: 0.0929 - val_accuracy: 0.9670\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1498 - accuracy: 0.9569 - val_loss: 0.0744 - val_accuracy: 0.9730\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1208 - accuracy: 0.9631 - val_loss: 0.0911 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1300 - accuracy: 0.9567 - val_loss: 0.0897 - val_accuracy: 0.9660\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1224 - accuracy: 0.9605 - val_loss: 0.0633 - val_accuracy: 0.9780\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0924 - accuracy: 0.9695 - val_loss: 0.0895 - val_accuracy: 0.9660\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1713 - accuracy: 0.9505 - val_loss: 0.0778 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0864 - accuracy: 0.9737 - val_loss: 0.0675 - val_accuracy: 0.9800\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0812 - accuracy: 0.9765 - val_loss: 0.0469 - val_accuracy: 0.9860\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0558 - accuracy: 0.9839 - val_loss: 0.0394 - val_accuracy: 0.9870\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0623 - accuracy: 0.9823 - val_loss: 0.0466 - val_accuracy: 0.9840\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0678 - accuracy: 0.9795 - val_loss: 0.0443 - val_accuracy: 0.9810\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 103ms/step - loss: 0.5105 - accuracy: 0.7213 - val_loss: 0.2572 - val_accuracy: 0.9380\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.2267 - accuracy: 0.9413 - val_loss: 0.1031 - val_accuracy: 0.9620\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.1396 - accuracy: 0.9533 - val_loss: 0.0988 - val_accuracy: 0.9610\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 12s 96ms/step - loss: 0.0948 - accuracy: 0.9671 - val_loss: 0.0750 - val_accuracy: 0.9700\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0950 - accuracy: 0.9683 - val_loss: 0.0950 - val_accuracy: 0.9590\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0826 - accuracy: 0.9773 - val_loss: 0.0587 - val_accuracy: 0.9830\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0731 - accuracy: 0.9794 - val_loss: 0.0583 - val_accuracy: 0.9800\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0856 - accuracy: 0.9720 - val_loss: 0.0646 - val_accuracy: 0.9790\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0743 - accuracy: 0.9803 - val_loss: 0.0620 - val_accuracy: 0.9810\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0989 - accuracy: 0.9756 - val_loss: 0.0448 - val_accuracy: 0.9840\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 97ms/step - loss: 0.0567 - accuracy: 0.9848 - val_loss: 0.0650 - val_accuracy: 0.9840\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1548 - accuracy: 0.9524 - val_loss: 0.1613 - val_accuracy: 0.9550\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 104ms/step - loss: 0.6402 - accuracy: 0.6385 - val_loss: 0.2131 - val_accuracy: 0.9670\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.2641 - accuracy: 0.9300 - val_loss: 0.1038 - val_accuracy: 0.9690\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1199 - accuracy: 0.9678 - val_loss: 0.1107 - val_accuracy: 0.9660\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.1253 - accuracy: 0.9596 - val_loss: 0.0962 - val_accuracy: 0.9640\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1165 - accuracy: 0.9616 - val_loss: 0.0747 - val_accuracy: 0.9810\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0783 - accuracy: 0.9785 - val_loss: 0.0605 - val_accuracy: 0.9830\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.0868 - accuracy: 0.9727 - val_loss: 0.0572 - val_accuracy: 0.9820\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.0883 - accuracy: 0.9735 - val_loss: 0.0492 - val_accuracy: 0.9850\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 107ms/step - loss: 0.0762 - accuracy: 0.9801 - val_loss: 0.0627 - val_accuracy: 0.9840\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 101ms/step - loss: 0.0729 - accuracy: 0.9786 - val_loss: 0.0983 - val_accuracy: 0.9740\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0840 - accuracy: 0.9763 - val_loss: 0.1408 - val_accuracy: 0.9570\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.1128 - accuracy: 0.9607 - val_loss: 0.0516 - val_accuracy: 0.9840\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 104ms/step - loss: 0.5676 - accuracy: 0.6791 - val_loss: 0.2469 - val_accuracy: 0.9540\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1808 - accuracy: 0.9520 - val_loss: 0.1345 - val_accuracy: 0.9590\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1343 - accuracy: 0.9592 - val_loss: 0.0942 - val_accuracy: 0.9730\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0975 - accuracy: 0.9673 - val_loss: 0.0781 - val_accuracy: 0.9680\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0890 - accuracy: 0.9733 - val_loss: 0.0570 - val_accuracy: 0.9810\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0861 - accuracy: 0.9753 - val_loss: 0.0711 - val_accuracy: 0.9760\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 98ms/step - loss: 0.0766 - accuracy: 0.9783 - val_loss: 0.0426 - val_accuracy: 0.9880\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0668 - accuracy: 0.9779 - val_loss: 0.0501 - val_accuracy: 0.9820\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.0641 - accuracy: 0.9817 - val_loss: 0.0488 - val_accuracy: 0.9800\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0646 - accuracy: 0.9801 - val_loss: 0.1396 - val_accuracy: 0.9580\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1213 - accuracy: 0.9677 - val_loss: 0.0471 - val_accuracy: 0.9830\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 13s 102ms/step - loss: 0.0737 - accuracy: 0.9782 - val_loss: 0.0399 - val_accuracy: 0.9870\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 15s 106ms/step - loss: 0.4998 - accuracy: 0.7256 - val_loss: 0.1632 - val_accuracy: 0.9580\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.1190 - accuracy: 0.9695 - val_loss: 0.0827 - val_accuracy: 0.9660\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.0951 - accuracy: 0.9687 - val_loss: 0.0875 - val_accuracy: 0.9600\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.1247 - accuracy: 0.9630 - val_loss: 0.1185 - val_accuracy: 0.9610\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.0842 - val_accuracy: 0.9720\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.1182 - accuracy: 0.9623 - val_loss: 0.0727 - val_accuracy: 0.9770\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.1461 - accuracy: 0.9503 - val_loss: 0.0585 - val_accuracy: 0.9790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 12s 100ms/step - loss: 0.0753 - accuracy: 0.9798 - val_loss: 0.0589 - val_accuracy: 0.9800\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 13s 100ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.0514 - val_accuracy: 0.9850\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 13s 104ms/step - loss: 0.0922 - accuracy: 0.9697 - val_loss: 0.2062 - val_accuracy: 0.9440\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 13s 103ms/step - loss: 0.2044 - accuracy: 0.9368 - val_loss: 0.0800 - val_accuracy: 0.9700\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 12s 99ms/step - loss: 0.0867 - accuracy: 0.9726 - val_loss: 0.0541 - val_accuracy: 0.9810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpQxqX5llI8n"
      },
      "source": [
        "Load in a second dataset for comparison (the data was taken from these sources: https://www.kaggle.com/shayanfazeli/heartbeat?select=ptbdb_abnormal.csv and https://www.kaggle.com/shayanfazeli/heartbeat?select=ptbdb_normal.csv). 2500 observations were taken from each and then combined into the dataset used below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "DoFfv4RmlLv3",
        "outputId": "c0ad2f8e-1f21-4b4e-d374-cf5b03d986fd"
      },
      "source": [
        "dataframe = pd.read_csv('https://raw.githubusercontent.com/chloeworthington/SCC413-Computer-Vision-Coursework/main/ecgdata.csv', header = None)\r\n",
        "raw_data = dataframe.values\r\n",
        "dataframe.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.932233</td>\n",
              "      <td>0.869679</td>\n",
              "      <td>0.886186</td>\n",
              "      <td>0.929626</td>\n",
              "      <td>0.908775</td>\n",
              "      <td>0.933970</td>\n",
              "      <td>0.801043</td>\n",
              "      <td>0.749783</td>\n",
              "      <td>0.687229</td>\n",
              "      <td>0.635100</td>\n",
              "      <td>0.649870</td>\n",
              "      <td>0.635100</td>\n",
              "      <td>0.655083</td>\n",
              "      <td>0.664639</td>\n",
              "      <td>0.633362</td>\n",
              "      <td>0.746308</td>\n",
              "      <td>0.871416</td>\n",
              "      <td>0.938314</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.948740</td>\n",
              "      <td>0.396177</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039096</td>\n",
              "      <td>0.128584</td>\n",
              "      <td>0.305821</td>\n",
              "      <td>0.640313</td>\n",
              "      <td>0.617724</td>\n",
              "      <td>0.537793</td>\n",
              "      <td>0.352737</td>\n",
              "      <td>0.220678</td>\n",
              "      <td>0.256299</td>\n",
              "      <td>0.357950</td>\n",
              "      <td>0.482189</td>\n",
              "      <td>0.570808</td>\n",
              "      <td>0.577758</td>\n",
              "      <td>0.620330</td>\n",
              "      <td>0.622068</td>\n",
              "      <td>0.618593</td>\n",
              "      <td>0.626412</td>\n",
              "      <td>0.612511</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.606941</td>\n",
              "      <td>0.384181</td>\n",
              "      <td>0.254237</td>\n",
              "      <td>0.223567</td>\n",
              "      <td>0.276836</td>\n",
              "      <td>0.253430</td>\n",
              "      <td>0.184826</td>\n",
              "      <td>0.153349</td>\n",
              "      <td>0.121872</td>\n",
              "      <td>0.125101</td>\n",
              "      <td>0.129136</td>\n",
              "      <td>0.137207</td>\n",
              "      <td>0.137207</td>\n",
              "      <td>0.132365</td>\n",
              "      <td>0.126715</td>\n",
              "      <td>0.158192</td>\n",
              "      <td>0.152542</td>\n",
              "      <td>0.132365</td>\n",
              "      <td>0.168684</td>\n",
              "      <td>0.156578</td>\n",
              "      <td>0.183212</td>\n",
              "      <td>0.191283</td>\n",
              "      <td>0.204197</td>\n",
              "      <td>0.230024</td>\n",
              "      <td>0.244552</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.307506</td>\n",
              "      <td>0.321227</td>\n",
              "      <td>0.327684</td>\n",
              "      <td>0.351897</td>\n",
              "      <td>0.373688</td>\n",
              "      <td>0.397094</td>\n",
              "      <td>0.363196</td>\n",
              "      <td>0.340597</td>\n",
              "      <td>0.307506</td>\n",
              "      <td>0.258273</td>\n",
              "      <td>0.197740</td>\n",
              "      <td>0.199354</td>\n",
              "      <td>0.173527</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.951613</td>\n",
              "      <td>0.923963</td>\n",
              "      <td>0.853303</td>\n",
              "      <td>0.791859</td>\n",
              "      <td>0.734255</td>\n",
              "      <td>0.672043</td>\n",
              "      <td>0.685100</td>\n",
              "      <td>0.670507</td>\n",
              "      <td>0.667435</td>\n",
              "      <td>0.681260</td>\n",
              "      <td>0.616743</td>\n",
              "      <td>0.624424</td>\n",
              "      <td>0.619816</td>\n",
              "      <td>0.596006</td>\n",
              "      <td>0.627496</td>\n",
              "      <td>0.631336</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.613671</td>\n",
              "      <td>0.607527</td>\n",
              "      <td>0.586790</td>\n",
              "      <td>0.568356</td>\n",
              "      <td>0.543779</td>\n",
              "      <td>0.526882</td>\n",
              "      <td>0.552995</td>\n",
              "      <td>0.577573</td>\n",
              "      <td>0.592166</td>\n",
              "      <td>0.576037</td>\n",
              "      <td>0.579109</td>\n",
              "      <td>0.573733</td>\n",
              "      <td>0.589862</td>\n",
              "      <td>0.542243</td>\n",
              "      <td>0.519201</td>\n",
              "      <td>0.514593</td>\n",
              "      <td>0.528418</td>\n",
              "      <td>0.493856</td>\n",
              "      <td>0.485407</td>\n",
              "      <td>0.461598</td>\n",
              "      <td>0.478495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.977819</td>\n",
              "      <td>0.899261</td>\n",
              "      <td>0.230129</td>\n",
              "      <td>0.032348</td>\n",
              "      <td>0.142329</td>\n",
              "      <td>0.223660</td>\n",
              "      <td>0.328096</td>\n",
              "      <td>0.367837</td>\n",
              "      <td>0.381701</td>\n",
              "      <td>0.389094</td>\n",
              "      <td>0.357671</td>\n",
              "      <td>0.379852</td>\n",
              "      <td>0.375231</td>\n",
              "      <td>0.397412</td>\n",
              "      <td>0.388170</td>\n",
              "      <td>0.378004</td>\n",
              "      <td>0.398336</td>\n",
              "      <td>0.419593</td>\n",
              "      <td>0.427911</td>\n",
              "      <td>0.402033</td>\n",
              "      <td>0.378928</td>\n",
              "      <td>0.400185</td>\n",
              "      <td>0.406654</td>\n",
              "      <td>0.422366</td>\n",
              "      <td>0.402033</td>\n",
              "      <td>0.383549</td>\n",
              "      <td>0.398336</td>\n",
              "      <td>0.393715</td>\n",
              "      <td>0.417745</td>\n",
              "      <td>0.388170</td>\n",
              "      <td>0.360444</td>\n",
              "      <td>0.402957</td>\n",
              "      <td>0.392791</td>\n",
              "      <td>0.413124</td>\n",
              "      <td>0.377079</td>\n",
              "      <td>0.327172</td>\n",
              "      <td>0.346580</td>\n",
              "      <td>0.338262</td>\n",
              "      <td>0.356747</td>\n",
              "      <td>0.314233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.935618</td>\n",
              "      <td>0.801661</td>\n",
              "      <td>0.805815</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.722741</td>\n",
              "      <td>0.480789</td>\n",
              "      <td>0.454829</td>\n",
              "      <td>0.319834</td>\n",
              "      <td>0.266874</td>\n",
              "      <td>0.308411</td>\n",
              "      <td>0.285566</td>\n",
              "      <td>0.343718</td>\n",
              "      <td>0.281412</td>\n",
              "      <td>0.281412</td>\n",
              "      <td>0.283489</td>\n",
              "      <td>0.281412</td>\n",
              "      <td>0.319834</td>\n",
              "      <td>0.311526</td>\n",
              "      <td>0.283489</td>\n",
              "      <td>0.278297</td>\n",
              "      <td>0.274143</td>\n",
              "      <td>0.317757</td>\n",
              "      <td>0.267913</td>\n",
              "      <td>0.275182</td>\n",
              "      <td>0.280374</td>\n",
              "      <td>0.255452</td>\n",
              "      <td>0.313603</td>\n",
              "      <td>0.266874</td>\n",
              "      <td>0.244029</td>\n",
              "      <td>0.198339</td>\n",
              "      <td>0.192108</td>\n",
              "      <td>0.190031</td>\n",
              "      <td>0.134995</td>\n",
              "      <td>0.086189</td>\n",
              "      <td>0.078920</td>\n",
              "      <td>0.036345</td>\n",
              "      <td>0.024922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044652</td>\n",
              "      <td>0.024922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 188 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4    ...  183  184  185  186  187\n",
              "0  0.932233  0.869679  0.886186  0.929626  0.908775  ...  0.0  0.0  0.0    0    1\n",
              "1  1.000000  0.606941  0.384181  0.254237  0.223567  ...  0.0  0.0  0.0    0    1\n",
              "2  1.000000  0.951613  0.923963  0.853303  0.791859  ...  0.0  0.0  0.0    0    1\n",
              "3  0.977819  0.899261  0.230129  0.032348  0.142329  ...  0.0  0.0  0.0    0    1\n",
              "4  0.935618  0.801661  0.805815  1.000000  0.722741  ...  0.0  0.0  0.0    0    1\n",
              "\n",
              "[5 rows x 188 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "KOS6GG7JlPM0",
        "outputId": "4f103af2-a779-4275-c646-2e5a2fb67578"
      },
      "source": [
        "# The last element contains the labels\r\n",
        "labels = raw_data[:, -1]\r\n",
        "\r\n",
        "# The other data points are the electrocadriogram data\r\n",
        "data = raw_data[:, 0:-1]\r\n",
        "\r\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=21)\r\n",
        "\r\n",
        "# Normalize to [0, 1]\r\n",
        "min_val = tf.reduce_min(train_data)\r\n",
        "max_val = tf.reduce_max(train_data)\r\n",
        "\r\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\r\n",
        "test_data = (test_data - min_val) / (max_val - min_val)\r\n",
        "\r\n",
        "train_data = tf.cast(train_data, tf.float32)\r\n",
        "test_data = tf.cast(test_data, tf.float32)\r\n",
        "\r\n",
        "# plot data\r\n",
        "plt.grid()\r\n",
        "plt.plot(np.arange(187), train_data[0])\r\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "CcDVFfLGlUD4",
        "outputId": "e336143c-656c-408d-ca29-240ecf7e4070"
      },
      "source": [
        "plt.grid()\r\n",
        "plt.plot(np.arange(187), train_data[100])\r\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Yn-iXzlX6D"
      },
      "source": [
        "# the model requires a 3-dimensional input therefore, we need to expand the \r\n",
        "# dimensions of the training and testing data.\r\n",
        "train_data = np.expand_dims(train_data, 2)\r\n",
        "test_data = np.expand_dims(test_data, 2)\r\n",
        "\r\n",
        "# the model requires these to be in binary matrix form.\r\n",
        "train_classes = to_categorical(train_labels, 2)\r\n",
        "test_classes = to_categorical(test_labels, 2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMPfn6EClbcq"
      },
      "source": [
        "Performance metrics for second data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXF4kbCblfCL",
        "outputId": "179897d1-0fea-416b-89b0-1269496e98ae"
      },
      "source": [
        "#initiate performance metrics lists.\r\n",
        "accuracy2 = []*10\r\n",
        "precision2 = []*10\r\n",
        "recall2 = []*10\r\n",
        "f1_2 = []*10\r\n",
        "\r\n",
        "#change input shape\r\n",
        "input_shape = (187, 1)\r\n",
        "\r\n",
        "#repeat 10 times for accuracy.\r\n",
        "for i in range(10):\r\n",
        "  #build model and predict for the test data.\r\n",
        "  model2 = buildmodel(12, 32)\r\n",
        "  #calculate and record performance metrics.\r\n",
        "  pred2 = model2.predict_classes(test_data)\r\n",
        "  accuracy2.append(accuracy_score(test_labels, pred2))\r\n",
        "  precision2.append(precision_score(test_labels, pred2))\r\n",
        "  recall2.append(recall_score(test_labels, pred2))\r\n",
        "  f1_2.append(f1_score(test_labels, pred2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6932 - accuracy: 0.5158 - val_loss: 0.6929 - val_accuracy: 0.5220\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6935 - accuracy: 0.5017 - val_loss: 0.6923 - val_accuracy: 0.5290\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6907 - val_accuracy: 0.5320\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 14s 113ms/step - loss: 0.6942 - accuracy: 0.4818 - val_loss: 0.6933 - val_accuracy: 0.4770\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6930 - accuracy: 0.5146 - val_loss: 0.6932 - val_accuracy: 0.4800\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6929 - accuracy: 0.5206 - val_loss: 0.6931 - val_accuracy: 0.5520\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6938 - accuracy: 0.4686 - val_loss: 0.6935 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6932 - accuracy: 0.5152 - val_loss: 0.7233 - val_accuracy: 0.5210\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.7021 - accuracy: 0.5191 - val_loss: 0.6943 - val_accuracy: 0.4780\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 120ms/step - loss: 0.6932 - accuracy: 0.4868 - val_loss: 0.6934 - val_accuracy: 0.4790\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6935 - accuracy: 0.5004 - val_loss: 0.6936 - val_accuracy: 0.4780\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6929 - accuracy: 0.5115 - val_loss: 0.6927 - val_accuracy: 0.5230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6936 - accuracy: 0.5023 - val_loss: 0.6927 - val_accuracy: 0.5210\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6939 - accuracy: 0.5025 - val_loss: 0.6929 - val_accuracy: 0.4770\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6937 - accuracy: 0.4877 - val_loss: 0.6934 - val_accuracy: 0.4790\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6934 - accuracy: 0.4808 - val_loss: 0.6939 - val_accuracy: 0.4790\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 120ms/step - loss: 0.6931 - accuracy: 0.5151 - val_loss: 0.6936 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6934 - accuracy: 0.5066 - val_loss: 0.6934 - val_accuracy: 0.4790\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6933 - accuracy: 0.5060 - val_loss: 0.6938 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6930 - accuracy: 0.5189 - val_loss: 0.6931 - val_accuracy: 0.5170\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6935 - val_accuracy: 0.4790\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6947 - val_accuracy: 0.4790\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6929 - accuracy: 0.5109 - val_loss: 0.6923 - val_accuracy: 0.5210\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6936 - accuracy: 0.5025 - val_loss: 0.6941 - val_accuracy: 0.4790\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6936 - accuracy: 0.4979 - val_loss: 0.6933 - val_accuracy: 0.4800\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6937 - accuracy: 0.4961 - val_loss: 0.6961 - val_accuracy: 0.4790\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6933 - accuracy: 0.5048 - val_loss: 0.6946 - val_accuracy: 0.4780\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6936 - accuracy: 0.5042 - val_loss: 0.6943 - val_accuracy: 0.4800\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6932 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4780\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6932 - accuracy: 0.5153 - val_loss: 0.6949 - val_accuracy: 0.4790\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6938 - accuracy: 0.4962 - val_loss: 0.6942 - val_accuracy: 0.4800\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6928 - accuracy: 0.5119 - val_loss: 0.6941 - val_accuracy: 0.4800\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6936 - accuracy: 0.4883 - val_loss: 0.6935 - val_accuracy: 0.4800\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6932 - accuracy: 0.5056 - val_loss: 0.6947 - val_accuracy: 0.4790\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6935 - accuracy: 0.5009 - val_loss: 0.6939 - val_accuracy: 0.4790\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6930 - accuracy: 0.5110 - val_loss: 0.6922 - val_accuracy: 0.5310\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6931 - accuracy: 0.4978 - val_loss: 0.6961 - val_accuracy: 0.4790\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6938 - accuracy: 0.4885 - val_loss: 0.6930 - val_accuracy: 0.5200\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6931 - accuracy: 0.5066 - val_loss: 0.6931 - val_accuracy: 0.4830\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6925 - accuracy: 0.5106 - val_loss: 0.6950 - val_accuracy: 0.4790\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6933 - accuracy: 0.5053 - val_loss: 0.6936 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6936 - accuracy: 0.4871 - val_loss: 0.6939 - val_accuracy: 0.4800\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6931 - val_accuracy: 0.4870\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6937 - val_accuracy: 0.4800\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6930 - accuracy: 0.5091 - val_loss: 0.6936 - val_accuracy: 0.4780\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6933 - accuracy: 0.4761 - val_loss: 0.6936 - val_accuracy: 0.4800\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6937 - val_accuracy: 0.4800\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6935 - accuracy: 0.5045 - val_loss: 0.6935 - val_accuracy: 0.4800\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6936 - accuracy: 0.5009 - val_loss: 0.6929 - val_accuracy: 0.5210\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 116ms/step - loss: 0.6933 - accuracy: 0.5140 - val_loss: 0.6928 - val_accuracy: 0.5220\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6935 - accuracy: 0.5043 - val_loss: 0.6928 - val_accuracy: 0.5210\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6931 - val_accuracy: 0.4840\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6944 - accuracy: 0.4861 - val_loss: 0.6967 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.7107 - accuracy: 0.4948 - val_loss: 0.6931 - val_accuracy: 0.5210\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6942 - accuracy: 0.5063 - val_loss: 0.6950 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6970 - accuracy: 0.4889 - val_loss: 0.6924 - val_accuracy: 0.5210\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6946 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.4790\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6928 - accuracy: 0.5221 - val_loss: 0.6971 - val_accuracy: 0.4790\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6939 - accuracy: 0.5066 - val_loss: 0.6951 - val_accuracy: 0.4790\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 121ms/step - loss: 0.6940 - accuracy: 0.4996 - val_loss: 0.6938 - val_accuracy: 0.4790\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 123ms/step - loss: 0.6936 - accuracy: 0.5019 - val_loss: 0.7031 - val_accuracy: 0.4790\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6846 - accuracy: 0.5406 - val_loss: 0.6916 - val_accuracy: 0.5850\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6584 - accuracy: 0.6121 - val_loss: 0.6923 - val_accuracy: 0.5210\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6954 - accuracy: 0.4964 - val_loss: 0.6947 - val_accuracy: 0.4790\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6937 - accuracy: 0.4981 - val_loss: 0.6919 - val_accuracy: 0.5210\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.7076 - accuracy: 0.5148 - val_loss: 0.6946 - val_accuracy: 0.4790\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6941 - accuracy: 0.5203 - val_loss: 0.6929 - val_accuracy: 0.5210\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6960 - accuracy: 0.5072 - val_loss: 0.6926 - val_accuracy: 0.5290\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6941 - accuracy: 0.5065 - val_loss: 0.6934 - val_accuracy: 0.4810\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 120ms/step - loss: 0.6939 - accuracy: 0.5094 - val_loss: 0.6922 - val_accuracy: 0.5210\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 124ms/step - loss: 0.6938 - accuracy: 0.4921 - val_loss: 0.6934 - val_accuracy: 0.4790\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 124ms/step - loss: 0.6935 - accuracy: 0.5111 - val_loss: 0.6938 - val_accuracy: 0.4790\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 122ms/step - loss: 0.6935 - accuracy: 0.4948 - val_loss: 0.6927 - val_accuracy: 0.5210\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6934 - accuracy: 0.4994 - val_loss: 0.6936 - val_accuracy: 0.4810\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6926 - accuracy: 0.5230 - val_loss: 0.6930 - val_accuracy: 0.5200\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6940 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4890\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 116ms/step - loss: 0.6900 - accuracy: 0.5558 - val_loss: 0.6939 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6929 - accuracy: 0.5159 - val_loss: 0.6929 - val_accuracy: 0.5160\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6936 - accuracy: 0.4839 - val_loss: 0.6937 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6931 - val_accuracy: 0.5150\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6930 - accuracy: 0.5087 - val_loss: 0.6929 - val_accuracy: 0.5170\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6931 - accuracy: 0.4981 - val_loss: 0.6936 - val_accuracy: 0.4790\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6935 - accuracy: 0.4988 - val_loss: 0.6930 - val_accuracy: 0.5190\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.4790\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 123ms/step - loss: 0.6935 - accuracy: 0.4844 - val_loss: 0.6934 - val_accuracy: 0.4860\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6931 - accuracy: 0.5007 - val_loss: 0.7015 - val_accuracy: 0.5210\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.7349 - accuracy: 0.4850 - val_loss: 0.6924 - val_accuracy: 0.5210\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6950 - accuracy: 0.4976 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6938 - accuracy: 0.4975 - val_loss: 0.6952 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6949 - accuracy: 0.5101 - val_loss: 0.6931 - val_accuracy: 0.5210\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6961 - accuracy: 0.5012 - val_loss: 0.6923 - val_accuracy: 0.5210\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6964 - accuracy: 0.4955 - val_loss: 0.6923 - val_accuracy: 0.5210\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 121ms/step - loss: 0.6953 - accuracy: 0.5005 - val_loss: 0.6937 - val_accuracy: 0.4790\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6947 - accuracy: 0.5009 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6944 - accuracy: 0.5024 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6938 - accuracy: 0.5084 - val_loss: 0.6930 - val_accuracy: 0.5210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 121ms/step - loss: 0.6936 - accuracy: 0.4938 - val_loss: 0.7059 - val_accuracy: 0.4790\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6950 - accuracy: 0.4952 - val_loss: 0.6939 - val_accuracy: 0.4790\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6932 - accuracy: 0.5066 - val_loss: 0.6931 - val_accuracy: 0.5170\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 14s 114ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6936 - val_accuracy: 0.4800\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6936 - val_accuracy: 0.4790\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 121ms/step - loss: 0.6933 - accuracy: 0.4975 - val_loss: 0.6937 - val_accuracy: 0.4790\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 16s 126ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6945 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6939 - accuracy: 0.4985 - val_loss: 0.6941 - val_accuracy: 0.4800\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6931 - accuracy: 0.5012 - val_loss: 0.6948 - val_accuracy: 0.4790\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 14s 115ms/step - loss: 0.6924 - accuracy: 0.5211 - val_loss: 0.6930 - val_accuracy: 0.5210\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6936 - accuracy: 0.5059 - val_loss: 0.6937 - val_accuracy: 0.4790\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6931 - accuracy: 0.4994 - val_loss: 0.6938 - val_accuracy: 0.4790\n",
            "Epoch 1/12\n",
            "125/125 [==============================] - 17s 124ms/step - loss: 0.6938 - accuracy: 0.4966 - val_loss: 0.6924 - val_accuracy: 0.5210\n",
            "Epoch 2/12\n",
            "125/125 [==============================] - 15s 121ms/step - loss: 0.6924 - accuracy: 0.5084 - val_loss: 0.6925 - val_accuracy: 0.5210\n",
            "Epoch 3/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6934 - accuracy: 0.4932 - val_loss: 0.6942 - val_accuracy: 0.4770\n",
            "Epoch 4/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6935 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4820\n",
            "Epoch 5/12\n",
            "125/125 [==============================] - 16s 128ms/step - loss: 0.6928 - accuracy: 0.5173 - val_loss: 0.6928 - val_accuracy: 0.5220\n",
            "Epoch 6/12\n",
            "125/125 [==============================] - 15s 118ms/step - loss: 0.6934 - accuracy: 0.4839 - val_loss: 0.6943 - val_accuracy: 0.4790\n",
            "Epoch 7/12\n",
            "125/125 [==============================] - 15s 117ms/step - loss: 0.6937 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4790\n",
            "Epoch 8/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6936 - accuracy: 0.4917 - val_loss: 0.6935 - val_accuracy: 0.4790\n",
            "Epoch 9/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6931 - accuracy: 0.5102 - val_loss: 0.6930 - val_accuracy: 0.4760\n",
            "Epoch 10/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6936 - val_accuracy: 0.4810\n",
            "Epoch 11/12\n",
            "125/125 [==============================] - 15s 119ms/step - loss: 0.6936 - accuracy: 0.5140 - val_loss: 0.6934 - val_accuracy: 0.4780\n",
            "Epoch 12/12\n",
            "125/125 [==============================] - 15s 120ms/step - loss: 0.6935 - accuracy: 0.4870 - val_loss: 0.6932 - val_accuracy: 0.5160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeqpKu_lkeA"
      },
      "source": [
        "Collate the performance metrics into a dataframe and save for later direct comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLEAL1alljT"
      },
      "source": [
        "lstmperf = pd.DataFrame()\r\n",
        "lstmperf[\"Accuracy1\"] = accuracy1\r\n",
        "lstmperf[\"Precision1\"] = precision1\r\n",
        "lstmperf[\"Recall1\"] = recall1\r\n",
        "lstmperf[\"F1Score1\"] = f1_1\r\n",
        "lstmperf[\"Accuracy2\"] = accuracy2\r\n",
        "lstmperf[\"Precision2\"] = precision2\r\n",
        "lstmperf[\"Recall2\"] = recall2\r\n",
        "lstmperf[\"F1Score2\"] = f1_2\r\n",
        "\r\n",
        "lstmperf.to_csv(\"LSTM_performance_metrics.csv\")"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}